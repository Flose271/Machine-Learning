{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7faac46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "import torch\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "47292add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>39</th>\n",
       "      <th>State-gov</th>\n",
       "      <th>77516</th>\n",
       "      <th>Bachelors</th>\n",
       "      <th>13</th>\n",
       "      <th>Never-married</th>\n",
       "      <th>Adm-clerical</th>\n",
       "      <th>Not-in-family</th>\n",
       "      <th>White</th>\n",
       "      <th>Male</th>\n",
       "      <th>2174</th>\n",
       "      <th>0</th>\n",
       "      <th>40</th>\n",
       "      <th>United-States</th>\n",
       "      <th>&lt;=50K</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>Private</td>\n",
       "      <td>284582</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   39          State-gov   77516   Bachelors   13        Never-married  \\\n",
       "0  50   Self-emp-not-inc   83311   Bachelors   13   Married-civ-spouse   \n",
       "1  38            Private  215646     HS-grad    9             Divorced   \n",
       "2  53            Private  234721        11th    7   Married-civ-spouse   \n",
       "3  28            Private  338409   Bachelors   13   Married-civ-spouse   \n",
       "4  37            Private  284582     Masters   14   Married-civ-spouse   \n",
       "\n",
       "         Adm-clerical   Not-in-family   White     Male   2174   0   40  \\\n",
       "0     Exec-managerial         Husband   White     Male      0   0   13   \n",
       "1   Handlers-cleaners   Not-in-family   White     Male      0   0   40   \n",
       "2   Handlers-cleaners         Husband   Black     Male      0   0   40   \n",
       "3      Prof-specialty            Wife   Black   Female      0   0   40   \n",
       "4     Exec-managerial            Wife   White   Female      0   0   40   \n",
       "\n",
       "    United-States   <=50K  \n",
       "0   United-States   <=50K  \n",
       "1   United-States   <=50K  \n",
       "2   United-States   <=50K  \n",
       "3            Cuba   <=50K  \n",
       "4   United-States   <=50K  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult = pd.read_csv('adult.csv')\n",
    "adult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "009442fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>2143</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>technician</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>29</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1506</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>unknown</td>\n",
       "      <td>single</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age           job  marital  education default  balance housing loan  \\\n",
       "0   58    management  married   tertiary      no     2143     yes   no   \n",
       "1   44    technician   single  secondary      no       29     yes   no   \n",
       "2   33  entrepreneur  married  secondary      no        2     yes  yes   \n",
       "3   47   blue-collar  married    unknown      no     1506     yes   no   \n",
       "4   33       unknown   single    unknown      no        1      no   no   \n",
       "\n",
       "   contact  day month  duration  campaign  pdays  previous poutcome   y  \n",
       "0  unknown    5   may       261         1     -1         0  unknown  no  \n",
       "1  unknown    5   may       151         1     -1         0  unknown  no  \n",
       "2  unknown    5   may        76         1     -1         0  unknown  no  \n",
       "3  unknown    5   may        92         1     -1         0  unknown  no  \n",
       "4  unknown    5   may       198         1     -1         0  unknown  no  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_marketing = pd.read_csv('bank-full.csv',delimiter=';')\n",
    "bank_marketing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3b2bc0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>gender</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>tenure</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>MultipleLines</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <th>...</th>\n",
       "      <th>DeviceProtection</th>\n",
       "      <th>TechSupport</th>\n",
       "      <th>StreamingTV</th>\n",
       "      <th>StreamingMovies</th>\n",
       "      <th>Contract</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7590-VHVEG</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>29.85</td>\n",
       "      <td>29.85</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5575-GNVDE</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>34</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>56.95</td>\n",
       "      <td>1889.5</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3668-QPYBK</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>53.85</td>\n",
       "      <td>108.15</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7795-CFOCW</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>45</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Bank transfer (automatic)</td>\n",
       "      <td>42.30</td>\n",
       "      <td>1840.75</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9237-HQITU</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>70.70</td>\n",
       "      <td>151.65</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerID  gender  SeniorCitizen Partner Dependents  tenure PhoneService  \\\n",
       "0  7590-VHVEG  Female              0     Yes         No       1           No   \n",
       "1  5575-GNVDE    Male              0      No         No      34          Yes   \n",
       "2  3668-QPYBK    Male              0      No         No       2          Yes   \n",
       "3  7795-CFOCW    Male              0      No         No      45           No   \n",
       "4  9237-HQITU  Female              0      No         No       2          Yes   \n",
       "\n",
       "      MultipleLines InternetService OnlineSecurity  ... DeviceProtection  \\\n",
       "0  No phone service             DSL             No  ...               No   \n",
       "1                No             DSL            Yes  ...              Yes   \n",
       "2                No             DSL            Yes  ...               No   \n",
       "3  No phone service             DSL            Yes  ...              Yes   \n",
       "4                No     Fiber optic             No  ...               No   \n",
       "\n",
       "  TechSupport StreamingTV StreamingMovies        Contract PaperlessBilling  \\\n",
       "0          No          No              No  Month-to-month              Yes   \n",
       "1          No          No              No        One year               No   \n",
       "2          No          No              No  Month-to-month              Yes   \n",
       "3         Yes          No              No        One year               No   \n",
       "4          No          No              No  Month-to-month              Yes   \n",
       "\n",
       "               PaymentMethod MonthlyCharges  TotalCharges Churn  \n",
       "0           Electronic check          29.85         29.85    No  \n",
       "1               Mailed check          56.95        1889.5    No  \n",
       "2               Mailed check          53.85        108.15   Yes  \n",
       "3  Bank transfer (automatic)          42.30       1840.75    No  \n",
       "4           Electronic check          70.70        151.65   Yes  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn = pd.read_csv('churn.csv')\n",
    "churn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7a44603a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
       "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
       "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
       "4   5            5.0           3.6            1.4           0.2  Iris-setosa"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = pd.read_csv('Iris.csv')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ecd1b483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tabnet(dataframe,x):\n",
    "    \n",
    "    #Prepare data\n",
    "    \n",
    "    data = dataframe.copy()\n",
    "    \n",
    "    target = data.columns[-1]\n",
    "    binary = data.nunique()[-1] == 2\n",
    "    print(binary)\n",
    "    \n",
    "    if \"Set\" not in data.columns:\n",
    "        data[\"Set\"] = np.random.choice([\"pretrain\",\"train\", \"valid\", \"test\"], \n",
    "                                       p =[0.8-x, x, .1, .1], size=(data.shape[0],))\n",
    "        \n",
    "    pretrain_indices = data[data.Set==\"pretrain\"].index\n",
    "    train_indices = data[data.Set==\"train\"].index\n",
    "    valid_indices = data[data.Set==\"valid\"].index\n",
    "    test_indices = data[data.Set==\"test\"].index\n",
    "    \n",
    "    #Data prep\n",
    "    \n",
    "    nunique = data.nunique()\n",
    "    types = data.dtypes\n",
    "\n",
    "    categorical_columns = []\n",
    "    categorical_dims =  {}\n",
    "    for col in data.columns:\n",
    "        if types[col] == 'object' or nunique[col] < 200:\n",
    "            l_enc = LabelEncoder()\n",
    "            data[col] = data[col].fillna(\"VV_likely\")\n",
    "            data[col] = l_enc.fit_transform(data[col].values)\n",
    "            categorical_columns.append(col)\n",
    "            categorical_dims[col] = len(l_enc.classes_)\n",
    "        else:\n",
    "            data.fillna(data.loc[train_indices, col].mean(), inplace=True)\n",
    "            \n",
    "    unused_feat = ['Set']\n",
    "    features = [ col for col in data.columns if col not in unused_feat+[target]] \n",
    "    cat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]\n",
    "    cat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]\n",
    "    \n",
    "    X_pretrain = data[features].values[pretrain_indices]\n",
    "    y_pretrain = data[target].values[pretrain_indices]\n",
    "    \n",
    "    X_train = data[features].values[train_indices]\n",
    "    y_train = data[target].values[train_indices]\n",
    "\n",
    "    X_valid = data[features].values[valid_indices]\n",
    "    y_valid = data[target].values[valid_indices]\n",
    "\n",
    "    X_test = data[features].values[test_indices]\n",
    "    y_test = data[target].values[test_indices]\n",
    "    \n",
    "    #Pre-training\n",
    "    \n",
    "    unsupervised_model = TabNetPretrainer(\n",
    "        cat_idxs=cat_idxs,\n",
    "        cat_dims=cat_dims,\n",
    "        cat_emb_dim=3,\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params=dict(lr=2e-2),\n",
    "        mask_type='entmax',verbose=0 # \"sparsemax\"\n",
    "    )\n",
    "    \n",
    "    max_epochs = 1000\n",
    "    \n",
    "    unsupervised_model.fit(\n",
    "    X_train=X_pretrain,\n",
    "    eval_set=[X_valid],\n",
    "    max_epochs=max_epochs, patience=5,\n",
    "    batch_size=2048, virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    drop_last=False,\n",
    "    pretraining_ratio=0.8,\n",
    "    )\n",
    "    \n",
    "    #Training\n",
    "    \n",
    "    clf = TabNetClassifier(optimizer_fn=torch.optim.Adam,\n",
    "                       optimizer_params=dict(lr=2e-2),\n",
    "                       scheduler_params={\"step_size\":10, # how to use learning rate scheduler\n",
    "                                         \"gamma\":0.9},\n",
    "                       scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                       mask_type='sparsemax',verbose=0 # This will be overwritten if using pretrain model\n",
    "                      )\n",
    "    \n",
    "    eval_metric = 'auc' if binary else 'accuracy'\n",
    "    \n",
    "    clf.fit(\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    eval_metric=[eval_metric],\n",
    "    max_epochs=max_epochs , patience=20,\n",
    "    batch_size=1024, virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    weights=1,\n",
    "    drop_last=False,\n",
    "    from_unsupervised=unsupervised_model\n",
    "    )\n",
    "    \n",
    "    if(binary):\n",
    "    \n",
    "        preds = clf.predict_proba(X_test)\n",
    "        test_auc = roc_auc_score(y_score=preds[:,1], y_true=y_test)\n",
    "\n",
    "        return test_auc\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        preds = clf.predict(X_test)\n",
    "        test_acc = accuracy_score(y_pred=preds, y_true=y_test)\n",
    "        \n",
    "        return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dac44552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xgboost(dataframe,x):\n",
    "    \n",
    "    data = dataframe.copy()\n",
    "    \n",
    "    target = data.columns[-1]\n",
    "    binary = data.nunique()[-1] == 2\n",
    "    print(binary)\n",
    "    \n",
    "    if \"Set\" not in data.columns:\n",
    "        data[\"Set\"] = np.random.choice([\"pretrain\",\"train\", \"valid\", \"test\"], \n",
    "                                       p =[0.8-x, x, .1, .1], size=(data.shape[0],))\n",
    "        \n",
    "    pretrain_indices = data[data.Set==\"pretrain\"].index\n",
    "    train_indices = data[data.Set==\"train\"].index\n",
    "    valid_indices = data[data.Set==\"valid\"].index\n",
    "    test_indices = data[data.Set==\"test\"].index\n",
    "    \n",
    "    nunique = data.nunique()\n",
    "    types = data.dtypes\n",
    "    \n",
    "    for col in data.columns:\n",
    "        if types[col] == 'object' or nunique[col] < 200:\n",
    "            l_enc = LabelEncoder()\n",
    "            data[col] = data[col].fillna(\"VV_likely\")\n",
    "            data[col] = l_enc.fit_transform(data[col].values)\n",
    "        else:\n",
    "            data.fillna(data.loc[train_indices, col].mean(), inplace=True)\n",
    "            \n",
    "    unused_feat = ['Set']\n",
    "    features = [ col for col in data.columns if col not in unused_feat+[target]] \n",
    "    \n",
    "    X_pretrain = data[features].values[pretrain_indices]\n",
    "    y_pretrain = data[target].values[pretrain_indices]\n",
    "    \n",
    "    X_train = data[features].values[train_indices]\n",
    "    y_train = data[target].values[train_indices]\n",
    "\n",
    "    X_valid = data[features].values[valid_indices]\n",
    "    y_valid = data[target].values[valid_indices]\n",
    "\n",
    "    X_test = data[features].values[test_indices]\n",
    "    y_test = data[target].values[test_indices]\n",
    "    \n",
    "    model = XGBClassifier()\n",
    "    \n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    if(binary):\n",
    "    \n",
    "        preds = model.predict_proba(X_test)\n",
    "        test_auc = roc_auc_score(y_score=preds[:,1], y_true=y_test)\n",
    "\n",
    "        return test_auc\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        preds = model.predict(X_test)\n",
    "        test_acc = accuracy_score(y_pred=preds, y_true=y_test)\n",
    "        \n",
    "        return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a7b5031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lightgbm(dataframe,x):\n",
    "    \n",
    "    data = dataframe.copy()\n",
    "    \n",
    "    target = data.columns[-1]\n",
    "    binary = data.nunique()[-1] == 2\n",
    "    print(binary)\n",
    "    \n",
    "    if \"Set\" not in data.columns:\n",
    "        data[\"Set\"] = np.random.choice([\"pretrain\",\"train\", \"valid\", \"test\"], \n",
    "                                       p =[0.8-x, x, .1, .1], size=(data.shape[0],))\n",
    "        \n",
    "    pretrain_indices = data[data.Set==\"pretrain\"].index\n",
    "    train_indices = data[data.Set==\"train\"].index\n",
    "    valid_indices = data[data.Set==\"valid\"].index\n",
    "    test_indices = data[data.Set==\"test\"].index\n",
    "    \n",
    "    nunique = data.nunique()\n",
    "    types = data.dtypes\n",
    "    \n",
    "    for col in data.columns:\n",
    "        if types[col] == 'object' or nunique[col] < 200:\n",
    "            l_enc = LabelEncoder()\n",
    "            data[col] = data[col].fillna(\"VV_likely\")\n",
    "            data[col] = l_enc.fit_transform(data[col].values)\n",
    "        else:\n",
    "            data.fillna(data.loc[train_indices, col].mean(), inplace=True)\n",
    "            \n",
    "    unused_feat = ['Set']\n",
    "    features = [ col for col in data.columns if col not in unused_feat+[target]] \n",
    "    \n",
    "    X_pretrain = data[features].values[pretrain_indices]\n",
    "    y_pretrain = data[target].values[pretrain_indices]\n",
    "    \n",
    "    X_train = data[features].values[train_indices]\n",
    "    y_train = data[target].values[train_indices]\n",
    "\n",
    "    X_valid = data[features].values[valid_indices]\n",
    "    y_valid = data[target].values[valid_indices]\n",
    "\n",
    "    X_test = data[features].values[test_indices]\n",
    "    y_test = data[target].values[test_indices]\n",
    "    \n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n",
    "    \n",
    "    params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'l2', 'l1'},\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "    }\n",
    "    \n",
    "    gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=20,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=5)\n",
    "    \n",
    "    if(binary):\n",
    "    \n",
    "        preds = gbm.predict(X_test)\n",
    "        test_auc = roc_auc_score(y_score=preds, y_true=y_test)\n",
    "    \n",
    "        return test_auc\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        probs = gbm.predict(X_test)\n",
    "        preds = [round(p) for p in probs]\n",
    "        test_acc = accuracy_score(y_pred=preds, y_true=y_test)\n",
    "        \n",
    "        return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0061089f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TabNet for 0.01 on Iris\n",
      "False\n",
      "\n",
      "Early stopping occurred at epoch 63 with best_epoch = 58 and best_val_0_unsup_loss = 1.09602\n",
      "Best weights from best epoch are automatically used!\n",
      "TabNet failed on Iris for value 0.01\n",
      "Starting XGBoost for 0.01 on Iris\n",
      "False\n",
      "[17:45:31] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.01 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "LightGBM failed on Iris for value 0.01\n",
      "Starting TabNet for 0.02 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 6 with best_epoch = 1 and best_val_0_unsup_loss = 1.6864\n",
      "Best weights from best epoch are automatically used!\n",
      "TabNet failed on Iris for value 0.02\n",
      "Starting XGBoost for 0.02 on Iris\n",
      "False\n",
      "[17:45:31] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.02 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 1\tvalid_0's l2: 1.66667\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's l1: 1\tvalid_0's l2: 1.66667\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's l1: 1\tvalid_0's l2: 1.66667\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's l1: 1\tvalid_0's l2: 1.66667\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's l1: 1\tvalid_0's l2: 1.66667\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's l1: 1\tvalid_0's l2: 1.66667\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 1\tvalid_0's l2: 1.66667\n",
      "Starting TabNet for 0.03 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 11 and best_val_0_unsup_loss = 1.12409\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 46 with best_epoch = 26 and best_valid_accuracy = 0.52941\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.03 on Iris\n",
      "False\n",
      "[17:45:32] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.03 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 1.375\tvalid_0's l2: 2.5\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's l1: 1.375\tvalid_0's l2: 2.5\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's l1: 1.375\tvalid_0's l2: 2.5\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's l1: 1.375\tvalid_0's l2: 2.5\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's l1: 1.375\tvalid_0's l2: 2.5\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's l1: 1.375\tvalid_0's l2: 2.5\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 1.375\tvalid_0's l2: 2.5\n",
      "Starting TabNet for 0.04 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 8 and best_val_0_unsup_loss = 1.58619\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 1 and best_valid_accuracy = 0.375\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.04 on Iris\n",
      "False\n",
      "[17:45:33] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.04 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 2.09412\tvalid_0's l2: 4.82824\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's l1: 2.09412\tvalid_0's l2: 4.82824\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's l1: 2.09412\tvalid_0's l2: 4.82824\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's l1: 2.09412\tvalid_0's l2: 4.82824\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's l1: 2.09412\tvalid_0's l2: 4.82824\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's l1: 2.09412\tvalid_0's l2: 4.82824\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 2.09412\tvalid_0's l2: 4.82824\n",
      "Starting TabNet for 0.05 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 31 with best_epoch = 26 and best_val_0_unsup_loss = 1.0078\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 13 and best_valid_accuracy = 0.5\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.05 on Iris\n",
      "False\n",
      "[17:45:35] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.05 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 0.8\tvalid_0's l2: 0.749091\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's l1: 0.8\tvalid_0's l2: 0.749091\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's l1: 0.8\tvalid_0's l2: 0.749091\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's l1: 0.8\tvalid_0's l2: 0.749091\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's l1: 0.8\tvalid_0's l2: 0.749091\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's l1: 0.8\tvalid_0's l2: 0.749091\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.8\tvalid_0's l2: 0.749091\n",
      "Starting TabNet for 0.06 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 28 and best_val_0_unsup_loss = 1.00748\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 13 and best_valid_accuracy = 0.44444\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.06 on Iris\n",
      "False\n",
      "[17:45:36] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.06 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 1\tvalid_0's l2: 1.83333\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's l1: 1\tvalid_0's l2: 1.83333\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's l1: 1\tvalid_0's l2: 1.83333\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's l1: 1\tvalid_0's l2: 1.83333\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's l1: 1\tvalid_0's l2: 1.83333\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's l1: 1\tvalid_0's l2: 1.83333\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 1\tvalid_0's l2: 1.83333\n",
      "Starting TabNet for 0.07 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 23 and best_val_0_unsup_loss = 1.01464\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_valid_accuracy = 0.5\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.07 on Iris\n",
      "False\n",
      "[17:45:37] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.07 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 1.03529\tvalid_0's l2: 1.53412\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's l1: 1.03529\tvalid_0's l2: 1.53412\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's l1: 1.03529\tvalid_0's l2: 1.53412\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's l1: 1.03529\tvalid_0's l2: 1.53412\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's l1: 1.03529\tvalid_0's l2: 1.53412\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's l1: 1.03529\tvalid_0's l2: 1.53412\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 1.03529\tvalid_0's l2: 1.53412\n",
      "Starting TabNet for 0.08 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 27 and best_val_0_unsup_loss = 1.02126\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 41 with best_epoch = 21 and best_valid_accuracy = 0.53333\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.08 on Iris\n",
      "False\n",
      "[17:45:39] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.08 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 0.834783\tvalid_0's l2: 0.96\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's l1: 0.834783\tvalid_0's l2: 0.96\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's l1: 0.834783\tvalid_0's l2: 0.96\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's l1: 0.834783\tvalid_0's l2: 0.96\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's l1: 0.834783\tvalid_0's l2: 0.96\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's l1: 0.834783\tvalid_0's l2: 0.96\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.834783\tvalid_0's l2: 0.96\n",
      "Starting TabNet for 0.09 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 25 and best_val_0_unsup_loss = 1.05533\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_valid_accuracy = 0.55\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.09 on Iris\n",
      "False\n",
      "[17:45:40] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.09 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 1.15686\tvalid_0's l2: 1.95425\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's l1: 1.15686\tvalid_0's l2: 1.95425\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's l1: 1.15686\tvalid_0's l2: 1.95425\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's l1: 1.15686\tvalid_0's l2: 1.95425\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's l1: 1.15686\tvalid_0's l2: 1.95425\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's l1: 1.15686\tvalid_0's l2: 1.95425\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 1.15686\tvalid_0's l2: 1.95425\n",
      "Starting TabNet for 0.1 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 17 and best_val_0_unsup_loss = 0.97558\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 36 with best_epoch = 16 and best_valid_accuracy = 0.47059\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.1 on Iris\n",
      "False\n",
      "[17:45:41] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.1 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 1.02222\tvalid_0's l2: 1.81037\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's l1: 1.02222\tvalid_0's l2: 1.81037\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's l1: 1.02222\tvalid_0's l2: 1.81037\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's l1: 1.02222\tvalid_0's l2: 1.81037\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's l1: 1.02222\tvalid_0's l2: 1.81037\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's l1: 1.02222\tvalid_0's l2: 1.81037\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 1.02222\tvalid_0's l2: 1.81037\n",
      "Starting TabNet for 0.11 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 19 and best_val_0_unsup_loss = 1.11031\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 40 with best_epoch = 20 and best_valid_accuracy = 0.55556\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.11 on Iris\n",
      "False\n",
      "[17:45:43] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.11 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 1.35315\tvalid_0's l2: 2.41985\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's l1: 1.35315\tvalid_0's l2: 2.41985\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's l1: 1.35315\tvalid_0's l2: 2.41985\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's l1: 1.35315\tvalid_0's l2: 2.41985\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's l1: 1.35315\tvalid_0's l2: 2.41985\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's l1: 1.35315\tvalid_0's l2: 2.41985\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 1.35315\tvalid_0's l2: 2.41985\n",
      "Starting TabNet for 0.12 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 31 with best_epoch = 26 and best_val_0_unsup_loss = 0.9723\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 2 and best_valid_accuracy = 0.6\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.12 on Iris\n",
      "False\n",
      "[17:45:44] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.12 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 1.48641\tvalid_0's l2: 2.79927\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's l1: 1.48641\tvalid_0's l2: 2.79927\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's l1: 1.48641\tvalid_0's l2: 2.79927\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's l1: 1.48641\tvalid_0's l2: 2.79927\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's l1: 1.48641\tvalid_0's l2: 2.79927\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's l1: 1.48641\tvalid_0's l2: 2.79927\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 1.48641\tvalid_0's l2: 2.79927\n",
      "Starting TabNet for 0.13 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 8 and best_val_0_unsup_loss = 1.0857\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 2 and best_valid_accuracy = 0.58333\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.13 on Iris\n",
      "False\n",
      "[17:45:45] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.13 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 1.78571\tvalid_0's l2: 3.82653\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's l1: 1.78571\tvalid_0's l2: 3.82653\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's l1: 1.78571\tvalid_0's l2: 3.82653\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's l1: 1.78571\tvalid_0's l2: 3.82653\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's l1: 1.78571\tvalid_0's l2: 3.82653\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's l1: 1.78571\tvalid_0's l2: 3.82653\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 1.78571\tvalid_0's l2: 3.82653\n",
      "Starting TabNet for 0.14 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 31 with best_epoch = 26 and best_val_0_unsup_loss = 1.01611\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_valid_accuracy = 0.52381\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.14 on Iris\n",
      "False\n",
      "[17:45:46] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.14 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 1.1886\tvalid_0's l2: 2.15582\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's l1: 1.1886\tvalid_0's l2: 2.15582\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's l1: 1.1886\tvalid_0's l2: 2.15582\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's l1: 1.1886\tvalid_0's l2: 2.15582\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's l1: 1.1886\tvalid_0's l2: 2.15582\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's l1: 1.1886\tvalid_0's l2: 2.15582\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 1.1886\tvalid_0's l2: 2.15582\n",
      "Starting TabNet for 0.15 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 13 and best_val_0_unsup_loss = 1.04702\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 31 with best_epoch = 11 and best_valid_accuracy = 0.58333\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.15 on Iris\n",
      "False\n",
      "[17:45:48] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.15 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 1.26061\tvalid_0's l2: 2.34913\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's l1: 1.26061\tvalid_0's l2: 2.34913\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's l1: 1.26061\tvalid_0's l2: 2.34913\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's l1: 1.26061\tvalid_0's l2: 2.34913\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's l1: 1.26061\tvalid_0's l2: 2.34913\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's l1: 1.26061\tvalid_0's l2: 2.34913\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 1.26061\tvalid_0's l2: 2.34913\n",
      "Starting TabNet for 0.16 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 20 and best_val_0_unsup_loss = 1.09327\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_valid_accuracy = 0.46667\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.16 on Iris\n",
      "False\n",
      "[17:45:49] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.16 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 1.08553\tvalid_0's l2: 2.03774\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's l1: 1.08553\tvalid_0's l2: 2.03774\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's l1: 1.08553\tvalid_0's l2: 2.03774\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's l1: 1.08553\tvalid_0's l2: 2.03774\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's l1: 1.08553\tvalid_0's l2: 2.03774\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's l1: 1.08553\tvalid_0's l2: 2.03774\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 1.08553\tvalid_0's l2: 2.03774\n",
      "Starting TabNet for 0.17 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 12 and best_val_0_unsup_loss = 1.07428\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 48 with best_epoch = 28 and best_valid_accuracy = 0.81818\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.17 on Iris\n",
      "False\n",
      "[17:45:51] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.17 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 1.16667\tvalid_0's l2: 1.88743\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's l1: 1.16667\tvalid_0's l2: 1.88743\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's l1: 1.16667\tvalid_0's l2: 1.88743\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's l1: 1.16667\tvalid_0's l2: 1.88743\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's l1: 1.16667\tvalid_0's l2: 1.88743\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's l1: 1.16667\tvalid_0's l2: 1.88743\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 1.16667\tvalid_0's l2: 1.88743\n",
      "Starting TabNet for 0.18 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 5 and best_val_0_unsup_loss = 1.18855\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 1 and best_valid_accuracy = 0.44444\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.18 on Iris\n",
      "False\n",
      "[17:45:51] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.18 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 0.92823\tvalid_0's l2: 1.26577\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's l1: 0.92823\tvalid_0's l2: 1.26577\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's l1: 0.92823\tvalid_0's l2: 1.26577\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's l1: 0.92823\tvalid_0's l2: 1.26577\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's l1: 0.92823\tvalid_0's l2: 1.26577\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's l1: 0.92823\tvalid_0's l2: 1.26577\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.92823\tvalid_0's l2: 1.26577\n",
      "Starting TabNet for 0.19 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 20 and best_val_0_unsup_loss = 1.05303\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 15 and best_valid_accuracy = 0.5\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.19 on Iris\n",
      "False\n",
      "[17:45:53] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.19 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 1.21212\tvalid_0's l2: 1.91552\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's l1: 1.21212\tvalid_0's l2: 1.91552\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's l1: 1.21212\tvalid_0's l2: 1.91552\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's l1: 1.21212\tvalid_0's l2: 1.91552\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's l1: 1.21212\tvalid_0's l2: 1.91552\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's l1: 1.21212\tvalid_0's l2: 1.91552\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 1.21212\tvalid_0's l2: 1.91552\n",
      "Starting TabNet for 0.2 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 9 with best_epoch = 4 and best_val_0_unsup_loss = 1.10163\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 49 with best_epoch = 29 and best_valid_accuracy = 0.72727\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.2 on Iris\n",
      "False\n",
      "[17:45:54] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.2 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 0.953846\tvalid_0's l2: 1.46982\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's l1: 0.953846\tvalid_0's l2: 1.46982\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's l1: 0.953846\tvalid_0's l2: 1.46982\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's l1: 0.953846\tvalid_0's l2: 1.46982\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's l1: 0.953846\tvalid_0's l2: 1.46982\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's l1: 0.953846\tvalid_0's l2: 1.46982\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.953846\tvalid_0's l2: 1.46982\n",
      "Starting TabNet for 0.21 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 55 with best_epoch = 50 and best_val_0_unsup_loss = 0.94054\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 41 with best_epoch = 21 and best_valid_accuracy = 0.5\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.21 on Iris\n",
      "False\n",
      "[17:45:56] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.21 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 1.07083\tvalid_0's l2: 1.95528\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's l1: 1.07083\tvalid_0's l2: 1.95528\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's l1: 1.07083\tvalid_0's l2: 1.95528\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's l1: 1.07083\tvalid_0's l2: 1.95528\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's l1: 1.07083\tvalid_0's l2: 1.95528\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's l1: 1.07083\tvalid_0's l2: 1.95528\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 1.07083\tvalid_0's l2: 1.95528\n",
      "Starting TabNet for 0.22 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 37 with best_epoch = 32 and best_val_0_unsup_loss = 1.02928\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 4 and best_valid_accuracy = 0.5\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.22 on Iris\n",
      "False\n",
      "[17:45:57] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.22 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 1.0251\tvalid_0's l2: 1.56574\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's l1: 1.0251\tvalid_0's l2: 1.56574\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's l1: 1.0251\tvalid_0's l2: 1.56574\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's l1: 1.0251\tvalid_0's l2: 1.56574\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's l1: 1.0251\tvalid_0's l2: 1.56574\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's l1: 1.0251\tvalid_0's l2: 1.56574\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 1.0251\tvalid_0's l2: 1.56574\n",
      "Starting TabNet for 0.23 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 10 and best_val_0_unsup_loss = 1.11236\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 49 with best_epoch = 29 and best_valid_accuracy = 0.66667\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.23 on Iris\n",
      "False\n",
      "[17:45:59] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.23 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 0.829837\tvalid_0's l2: 1.17384\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's l1: 0.829837\tvalid_0's l2: 1.17384\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's l1: 0.829837\tvalid_0's l2: 1.17384\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's l1: 0.829837\tvalid_0's l2: 1.17384\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's l1: 0.829837\tvalid_0's l2: 1.17384\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's l1: 0.829837\tvalid_0's l2: 1.17384\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.829837\tvalid_0's l2: 1.17384\n",
      "Starting TabNet for 0.24 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 8 with best_epoch = 3 and best_val_0_unsup_loss = 1.1732\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_accuracy = 0.5\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.24 on Iris\n",
      "False\n",
      "[17:46:00] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.24 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 1.09774\tvalid_0's l2: 1.77048\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's l1: 1.09774\tvalid_0's l2: 1.77048\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's l1: 1.09774\tvalid_0's l2: 1.77048\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's l1: 1.09774\tvalid_0's l2: 1.77048\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's l1: 1.09774\tvalid_0's l2: 1.77048\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's l1: 1.09774\tvalid_0's l2: 1.77048\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 1.09774\tvalid_0's l2: 1.77048\n",
      "Starting TabNet for 0.25 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 58 with best_epoch = 53 and best_val_0_unsup_loss = 1.00625\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 36 with best_epoch = 16 and best_valid_accuracy = 0.63636\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.25 on Iris\n",
      "False\n",
      "[17:46:02] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.25 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000166 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 1.11765\tvalid_0's l2: 1.82353\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's l1: 1.11765\tvalid_0's l2: 1.82353\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's l1: 1.11765\tvalid_0's l2: 1.82353\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's l1: 1.11765\tvalid_0's l2: 1.82353\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's l1: 1.11765\tvalid_0's l2: 1.82353\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's l1: 1.11765\tvalid_0's l2: 1.82353\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 1.11765\tvalid_0's l2: 1.82353\n",
      "Starting TabNet for 0.26 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 22 and best_val_0_unsup_loss = 1.02724\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 10 and best_valid_accuracy = 0.66667\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.26 on Iris\n",
      "False\n",
      "[17:46:03] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.26 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 1.1834\tvalid_0's l2: 1.94308\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's l1: 1.1834\tvalid_0's l2: 1.94308\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's l1: 1.1834\tvalid_0's l2: 1.94308\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's l1: 1.1834\tvalid_0's l2: 1.94308\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's l1: 1.1834\tvalid_0's l2: 1.94308\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's l1: 1.1834\tvalid_0's l2: 1.94308\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 1.1834\tvalid_0's l2: 1.94308\n",
      "Starting TabNet for 0.27 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 11 and best_val_0_unsup_loss = 1.06456\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 50 with best_epoch = 30 and best_valid_accuracy = 0.64706\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.27 on Iris\n",
      "False\n",
      "[17:46:04] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.27 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 0.894737\tvalid_0's l2: 1.14294\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's l1: 0.894737\tvalid_0's l2: 1.14294\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's l1: 0.894737\tvalid_0's l2: 1.14294\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's l1: 0.894737\tvalid_0's l2: 1.14294\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's l1: 0.894737\tvalid_0's l2: 1.14294\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's l1: 0.894737\tvalid_0's l2: 1.14294\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.894737\tvalid_0's l2: 1.14294\n",
      "Starting TabNet for 0.28 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 15 and best_val_0_unsup_loss = 1.13221\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_valid_accuracy = 0.5\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.28 on Iris\n",
      "False\n",
      "[17:46:05] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.28 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 1.31622\tvalid_0's l2: 2.32243\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's l1: 1.31622\tvalid_0's l2: 2.32243\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's l1: 1.31622\tvalid_0's l2: 2.32243\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's l1: 1.31622\tvalid_0's l2: 2.32243\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's l1: 1.31622\tvalid_0's l2: 2.32243\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's l1: 1.31622\tvalid_0's l2: 2.32243\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 1.31622\tvalid_0's l2: 2.32243\n",
      "Starting TabNet for 0.29 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 13 and best_val_0_unsup_loss = 1.14258\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_accuracy = 0.61538\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.29 on Iris\n",
      "False\n",
      "[17:46:06] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.29 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 0.914141\tvalid_0's l2: 1.33655\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's l1: 0.914141\tvalid_0's l2: 1.33655\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's l1: 0.914141\tvalid_0's l2: 1.33655\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's l1: 0.914141\tvalid_0's l2: 1.33655\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's l1: 0.914141\tvalid_0's l2: 1.33655\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's l1: 0.914141\tvalid_0's l2: 1.33655\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.914141\tvalid_0's l2: 1.33655\n",
      "Starting TabNet for 0.3 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 11 and best_val_0_unsup_loss = 1.05343\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 54 with best_epoch = 34 and best_valid_accuracy = 0.73333\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.3 on Iris\n",
      "False\n",
      "[17:46:08] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.3 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000214 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 0.957995\tvalid_0's l2: 1.30111\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's l1: 0.957995\tvalid_0's l2: 1.30111\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's l1: 0.957995\tvalid_0's l2: 1.30111\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's l1: 0.957995\tvalid_0's l2: 1.30111\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's l1: 0.957995\tvalid_0's l2: 1.30111\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's l1: 0.957995\tvalid_0's l2: 1.30111\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.957995\tvalid_0's l2: 1.30111\n",
      "Starting TabNet for 0.31 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 47 with best_epoch = 42 and best_val_0_unsup_loss = 1.02124\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_valid_accuracy = 0.57143\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.31 on Iris\n",
      "False\n",
      "[17:46:10] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.31 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000023 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 0.754663\tvalid_0's l2: 1.01242\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[2]\tvalid_0's l1: 0.754663\tvalid_0's l2: 1.01242\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[3]\tvalid_0's l1: 0.754663\tvalid_0's l2: 1.01242\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[4]\tvalid_0's l1: 0.754663\tvalid_0's l2: 1.01242\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[5]\tvalid_0's l1: 0.754663\tvalid_0's l2: 1.01242\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[6]\tvalid_0's l1: 0.754663\tvalid_0's l2: 1.01242\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l1: 0.754663\tvalid_0's l2: 1.01242\n",
      "Starting TabNet for 0.32 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 13 and best_val_0_unsup_loss = 1.20801\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 36 with best_epoch = 16 and best_valid_accuracy = 0.63636\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.32 on Iris\n",
      "False\n",
      "[17:46:11] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.32 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[1]\tvalid_0's l1: 1\tvalid_0's l2: 1.52632\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.966396\tvalid_0's l2: 1.40589\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.935004\tvalid_0's l2: 1.29847\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.902639\tvalid_0's l2: 1.19775\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.87529\tvalid_0's l2: 1.11295\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.848046\tvalid_0's l2: 1.0318\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.822816\tvalid_0's l2: 0.961098\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.796007\tvalid_0's l2: 0.892402\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.773171\tvalid_0's l2: 0.834228\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.748883\tvalid_0's l2: 0.777643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.72581\tvalid_0's l2: 0.726466\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[12]\tvalid_0's l1: 0.72581\tvalid_0's l2: 0.726466\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[13]\tvalid_0's l1: 0.72581\tvalid_0's l2: 0.726466\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.704729\tvalid_0's l2: 0.677638\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.684702\tvalid_0's l2: 0.633402\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.665677\tvalid_0's l2: 0.593319\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.647603\tvalid_0's l2: 0.556991\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.630433\tvalid_0's l2: 0.524061\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[19]\tvalid_0's l1: 0.630433\tvalid_0's l2: 0.524061\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.615386\tvalid_0's l2: 0.49658\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.615386\tvalid_0's l2: 0.49658\n",
      "Starting TabNet for 0.33 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 17 and best_val_0_unsup_loss = 1.07895\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_valid_accuracy = 0.57143\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.33 on Iris\n",
      "False\n",
      "[17:46:12] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.33 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000197 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.522563\tvalid_0's l2: 0.492065\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.507944\tvalid_0's l2: 0.453613\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.492898\tvalid_0's l2: 0.417556\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.479569\tvalid_0's l2: 0.384274\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.459658\tvalid_0's l2: 0.354994\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.448143\tvalid_0's l2: 0.33175\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.443813\tvalid_0's l2: 0.311168\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.417073\tvalid_0's l2: 0.286794\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.411141\tvalid_0's l2: 0.268108\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.398404\tvalid_0's l2: 0.25203\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.394667\tvalid_0's l2: 0.235869\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.392245\tvalid_0's l2: 0.221715\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.378572\tvalid_0's l2: 0.206789\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.376916\tvalid_0's l2: 0.195738\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.369207\tvalid_0's l2: 0.183072\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.359191\tvalid_0's l2: 0.169712\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.348373\tvalid_0's l2: 0.160553\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.33919\tvalid_0's l2: 0.149098\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.329273\tvalid_0's l2: 0.141407\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.320855\tvalid_0's l2: 0.13157\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.320855\tvalid_0's l2: 0.13157\n",
      "Starting TabNet for 0.34 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 27 and best_val_0_unsup_loss = 1.19238\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 6 and best_valid_accuracy = 0.66667\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.34 on Iris\n",
      "False\n",
      "[17:46:13] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.34 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000077 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.712167\tvalid_0's l2: 0.667068\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.692026\tvalid_0's l2: 0.619017\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.669981\tvalid_0's l2: 0.575114\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.645877\tvalid_0's l2: 0.534052\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.627219\tvalid_0's l2: 0.497226\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.605976\tvalid_0's l2: 0.461303\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.583789\tvalid_0's l2: 0.42939\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.565299\tvalid_0's l2: 0.399429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.54138\tvalid_0's l2: 0.368979\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.525513\tvalid_0's l2: 0.343515\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.511933\tvalid_0's l2: 0.327905\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.500197\tvalid_0's l2: 0.314057\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.483612\tvalid_0's l2: 0.294994\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.474647\tvalid_0's l2: 0.283712\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.461063\tvalid_0's l2: 0.26756\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.448247\tvalid_0's l2: 0.25138\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.436737\tvalid_0's l2: 0.236678\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.427844\tvalid_0's l2: 0.227891\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.41762\tvalid_0's l2: 0.215079\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.409004\tvalid_0's l2: 0.207732\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.409004\tvalid_0's l2: 0.207732\n",
      "Starting TabNet for 0.35 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 15 and best_val_0_unsup_loss = 1.20448\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 14 and best_valid_accuracy = 0.3\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.35 on Iris\n",
      "False\n",
      "[17:46:14] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.35 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.585863\tvalid_0's l2: 0.553469\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.559713\tvalid_0's l2: 0.512513\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.542921\tvalid_0's l2: 0.477092\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.537509\tvalid_0's l2: 0.445316\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.52191\tvalid_0's l2: 0.414852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.511733\tvalid_0's l2: 0.385769\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.502065\tvalid_0's l2: 0.359961\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.492851\tvalid_0's l2: 0.338152\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.470607\tvalid_0's l2: 0.314049\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.460468\tvalid_0's l2: 0.29535\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.454003\tvalid_0's l2: 0.279589\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.446668\tvalid_0's l2: 0.264415\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.436005\tvalid_0's l2: 0.250615\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.432572\tvalid_0's l2: 0.238764\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.422687\tvalid_0's l2: 0.227334\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.416531\tvalid_0's l2: 0.216837\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.407619\tvalid_0's l2: 0.207673\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.40194\tvalid_0's l2: 0.19898\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.39376\tvalid_0's l2: 0.191475\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.388489\tvalid_0's l2: 0.184243\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.388489\tvalid_0's l2: 0.184243\n",
      "Starting TabNet for 0.36 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 14 and best_val_0_unsup_loss = 1.04873\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 7 and best_valid_accuracy = 0.53333\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.36 on Iris\n",
      "False\n",
      "[17:46:16] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.36 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000029 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.604792\tvalid_0's l2: 0.584817\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.594884\tvalid_0's l2: 0.546467\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.587396\tvalid_0's l2: 0.51209\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.575141\tvalid_0's l2: 0.480231\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.563058\tvalid_0's l2: 0.448623\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.541439\tvalid_0's l2: 0.41446\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.530634\tvalid_0's l2: 0.390158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.510741\tvalid_0's l2: 0.361147\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.500802\tvalid_0's l2: 0.340566\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.482497\tvalid_0's l2: 0.315924\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.470359\tvalid_0's l2: 0.297609\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.459052\tvalid_0's l2: 0.281945\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.448053\tvalid_0's l2: 0.266356\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.43732\tvalid_0's l2: 0.252104\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.429201\tvalid_0's l2: 0.238931\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.42294\tvalid_0's l2: 0.230553\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.415399\tvalid_0's l2: 0.218892\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.409337\tvalid_0's l2: 0.211966\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.402679\tvalid_0's l2: 0.202015\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.396904\tvalid_0's l2: 0.196308\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.396904\tvalid_0's l2: 0.196308\n",
      "Starting TabNet for 0.37 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 20 and best_val_0_unsup_loss = 1.02242\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 6 and best_valid_accuracy = 0.69231\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.37 on Iris\n",
      "False\n",
      "[17:46:17] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.37 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.670916\tvalid_0's l2: 0.64413\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.637254\tvalid_0's l2: 0.598696\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.605745\tvalid_0's l2: 0.557699\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.593398\tvalid_0's l2: 0.520705\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.581668\tvalid_0's l2: 0.487324\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.571182\tvalid_0's l2: 0.456724\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.561221\tvalid_0's l2: 0.429151\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.546666\tvalid_0's l2: 0.40165\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.53273\tvalid_0's l2: 0.376106\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.523984\tvalid_0's l2: 0.355107\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.51592\tvalid_0's l2: 0.337563\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.5043\tvalid_0's l2: 0.319304\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.496897\tvalid_0's l2: 0.304591\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.486222\tvalid_0's l2: 0.28921\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.477942\tvalid_0's l2: 0.275644\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.467985\tvalid_0's l2: 0.26207\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.458525\tvalid_0's l2: 0.249787\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.449455\tvalid_0's l2: 0.23849\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.44073\tvalid_0's l2: 0.22797\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.432365\tvalid_0's l2: 0.218297\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.432365\tvalid_0's l2: 0.218297\n",
      "Starting TabNet for 0.38 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 25 and best_val_0_unsup_loss = 1.01172\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_accuracy = 0.73333\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.38 on Iris\n",
      "False\n",
      "[17:46:18] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.38 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.685115\tvalid_0's l2: 0.652947\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.658069\tvalid_0's l2: 0.608853\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.639571\tvalid_0's l2: 0.568078\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.617677\tvalid_0's l2: 0.530424\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.60026\tvalid_0's l2: 0.495508\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.58697\tvalid_0's l2: 0.464547\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.568828\tvalid_0's l2: 0.435727\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.555668\tvalid_0's l2: 0.408821\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.538909\tvalid_0's l2: 0.384163\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.526753\tvalid_0's l2: 0.361105\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.51179\tvalid_0's l2: 0.340616\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.497472\tvalid_0's l2: 0.32184\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.489459\tvalid_0's l2: 0.304711\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.472493\tvalid_0's l2: 0.287358\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.465354\tvalid_0's l2: 0.272542\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.450946\tvalid_0's l2: 0.258056\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.442296\tvalid_0's l2: 0.244872\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.431284\tvalid_0's l2: 0.232629\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.423229\tvalid_0's l2: 0.221143\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.412933\tvalid_0's l2: 0.210783\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.412933\tvalid_0's l2: 0.210783\n",
      "Starting TabNet for 0.39 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 54 with best_epoch = 49 and best_val_0_unsup_loss = 1.02641\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 63 with best_epoch = 43 and best_valid_accuracy = 0.66667\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.39 on Iris\n",
      "False\n",
      "[17:46:21] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.39 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000032 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.775703\tvalid_0's l2: 0.74267\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.740657\tvalid_0's l2: 0.689222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.707363\tvalid_0's l2: 0.640763\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.675734\tvalid_0's l2: 0.596817\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.650428\tvalid_0's l2: 0.556956\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.633328\tvalid_0's l2: 0.522398\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.613793\tvalid_0's l2: 0.48843\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.598169\tvalid_0's l2: 0.459341\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.580317\tvalid_0's l2: 0.430677\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.566042\tvalid_0's l2: 0.406176\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.550416\tvalid_0's l2: 0.382796\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.537338\tvalid_0's l2: 0.362054\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.523038\tvalid_0's l2: 0.342065\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.511671\tvalid_0's l2: 0.325206\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.498577\tvalid_0's l2: 0.308136\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.483236\tvalid_0's l2: 0.289761\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.468662\tvalid_0's l2: 0.272982\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.454817\tvalid_0's l2: 0.257653\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.441664\tvalid_0's l2: 0.243641\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.428863\tvalid_0's l2: 0.231079\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.428863\tvalid_0's l2: 0.231079\n",
      "Starting TabNet for 0.4 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 52 with best_epoch = 47 and best_val_0_unsup_loss = 1.06947\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 59 with best_epoch = 39 and best_valid_accuracy = 0.6875\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.4 on Iris\n",
      "False\n",
      "[17:46:23] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.4 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000248 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.507775\tvalid_0's l2: 0.420816\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.471686\tvalid_0's l2: 0.384144\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.437402\tvalid_0's l2: 0.351736\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.404831\tvalid_0's l2: 0.323141\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.382538\tvalid_0's l2: 0.297955\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.379069\tvalid_0's l2: 0.276484\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.374876\tvalid_0's l2: 0.258171\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.367347\tvalid_0's l2: 0.240976\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.362537\tvalid_0's l2: 0.225335\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.356016\tvalid_0's l2: 0.211628\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.354692\tvalid_0's l2: 0.199589\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.349641\tvalid_0's l2: 0.188709\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.347959\tvalid_0's l2: 0.179168\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.34336\tvalid_0's l2: 0.17064\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.34184\tvalid_0's l2: 0.163238\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.33753\tvalid_0's l2: 0.155712\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.331944\tvalid_0's l2: 0.148652\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.332189\tvalid_0's l2: 0.143688\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.328424\tvalid_0's l2: 0.138673\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.328225\tvalid_0's l2: 0.134926\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.328225\tvalid_0's l2: 0.134926\n",
      "Starting TabNet for 0.41 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 39 with best_epoch = 34 and best_val_0_unsup_loss = 1.14406\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 54 with best_epoch = 34 and best_valid_accuracy = 0.6087\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.41 on Iris\n",
      "False\n",
      "[17:46:25] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.41 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000161 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.620891\tvalid_0's l2: 0.601864\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.609262\tvalid_0's l2: 0.561345\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.587244\tvalid_0's l2: 0.519764\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.573445\tvalid_0's l2: 0.481395\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.561873\tvalid_0's l2: 0.450627\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.54952\tvalid_0's l2: 0.423202\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.538533\tvalid_0's l2: 0.398408\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.528095\tvalid_0's l2: 0.375991\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.515945\tvalid_0's l2: 0.352211\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.506363\tvalid_0's l2: 0.333422\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.489877\tvalid_0's l2: 0.311211\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.474214\tvalid_0's l2: 0.291233\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.465143\tvalid_0's l2: 0.27387\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.45716\tvalid_0's l2: 0.258378\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.445044\tvalid_0's l2: 0.24253\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.433094\tvalid_0's l2: 0.227534\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.421742\tvalid_0's l2: 0.214091\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.414795\tvalid_0's l2: 0.202792\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.404348\tvalid_0's l2: 0.191546\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.397953\tvalid_0's l2: 0.182204\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.397953\tvalid_0's l2: 0.182204\n",
      "Starting TabNet for 0.42 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 37 with best_epoch = 32 and best_val_0_unsup_loss = 1.08589\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 10 and best_valid_accuracy = 0.57143\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.42 on Iris\n",
      "False\n",
      "[17:46:26] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.42 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000247 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.740067\tvalid_0's l2: 0.691574\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.710482\tvalid_0's l2: 0.640552\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.685131\tvalid_0's l2: 0.593359\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.657894\tvalid_0's l2: 0.549866\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.634551\tvalid_0's l2: 0.509579\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.610221\tvalid_0's l2: 0.473406\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.587107\tvalid_0's l2: 0.440727\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.567655\tvalid_0's l2: 0.410297\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.549569\tvalid_0's l2: 0.382512\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.532923\tvalid_0's l2: 0.356627\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.515208\tvalid_0's l2: 0.33525\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.49926\tvalid_0's l2: 0.31388\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.486169\tvalid_0's l2: 0.295553\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.471152\tvalid_0's l2: 0.277684\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.457016\tvalid_0's l2: 0.260483\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.442498\tvalid_0's l2: 0.24404\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.428706\tvalid_0's l2: 0.229119\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.415604\tvalid_0's l2: 0.215576\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.403157\tvalid_0's l2: 0.20328\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.393065\tvalid_0's l2: 0.191965\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.393065\tvalid_0's l2: 0.191965\n",
      "Starting TabNet for 0.43 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 36 with best_epoch = 31 and best_val_0_unsup_loss = 0.99755\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 86 with best_epoch = 66 and best_valid_accuracy = 0.75\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.43 on Iris\n",
      "False\n",
      "[17:46:29] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.43 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.834822\tvalid_0's l2: 0.805367\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.806176\tvalid_0's l2: 0.741463\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.774986\tvalid_0's l2: 0.685482\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.748637\tvalid_0's l2: 0.631683\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.719934\tvalid_0's l2: 0.584464\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.692873\tvalid_0's l2: 0.540147\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.667436\tvalid_0's l2: 0.500238\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.642907\tvalid_0's l2: 0.463372\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.620608\tvalid_0's l2: 0.431606\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.597913\tvalid_0's l2: 0.399935\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.575834\tvalid_0's l2: 0.370798\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.554859\tvalid_0's l2: 0.344334\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.534933\tvalid_0's l2: 0.320292\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.517711\tvalid_0's l2: 0.298444\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.502271\tvalid_0's l2: 0.277507\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.485556\tvalid_0's l2: 0.260983\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.471647\tvalid_0's l2: 0.244551\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.45864\tvalid_0's l2: 0.228636\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.443834\tvalid_0's l2: 0.215416\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.431466\tvalid_0's l2: 0.202299\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.431466\tvalid_0's l2: 0.202299\n",
      "Starting TabNet for 0.44 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 14 and best_val_0_unsup_loss = 1.03271\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_valid_accuracy = 0.68421\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.44 on Iris\n",
      "False\n",
      "[17:46:30] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.44 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.741521\tvalid_0's l2: 0.705209\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.704966\tvalid_0's l2: 0.651754\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.68368\tvalid_0's l2: 0.600409\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.650106\tvalid_0's l2: 0.555149\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.630556\tvalid_0's l2: 0.511575\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.600823\tvalid_0's l2: 0.474471\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.572577\tvalid_0's l2: 0.440895\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.550721\tvalid_0's l2: 0.404945\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.535183\tvalid_0's l2: 0.374866\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.51033\tvalid_0's l2: 0.348561\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.487851\tvalid_0's l2: 0.325777\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.475447\tvalid_0's l2: 0.302578\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.458802\tvalid_0's l2: 0.281053\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.442985\tvalid_0's l2: 0.262857\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.432706\tvalid_0's l2: 0.244606\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.415076\tvalid_0's l2: 0.224445\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.398327\tvalid_0's l2: 0.206168\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.382416\tvalid_0's l2: 0.189597\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.3673\tvalid_0's l2: 0.174567\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.352939\tvalid_0's l2: 0.160934\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.352939\tvalid_0's l2: 0.160934\n",
      "Starting TabNet for 0.45 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 46 with best_epoch = 41 and best_val_0_unsup_loss = 1.02889\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 56 with best_epoch = 36 and best_valid_accuracy = 0.80952\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.45 on Iris\n",
      "False\n",
      "[17:46:32] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.45 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.542801\tvalid_0's l2: 0.50262\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.514651\tvalid_0's l2: 0.453552\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.487909\tvalid_0's l2: 0.409275\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.462503\tvalid_0's l2: 0.369323\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.438368\tvalid_0's l2: 0.333273\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.420024\tvalid_0's l2: 0.303652\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.40262\tvalid_0's l2: 0.276318\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.387887\tvalid_0's l2: 0.252271\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.373891\tvalid_0's l2: 0.230696\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.358859\tvalid_0's l2: 0.210474\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.342758\tvalid_0's l2: 0.191196\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.328839\tvalid_0's l2: 0.173835\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.315617\tvalid_0's l2: 0.158202\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.303057\tvalid_0's l2: 0.144322\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.291125\tvalid_0's l2: 0.131559\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.279679\tvalid_0's l2: 0.123253\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.273514\tvalid_0's l2: 0.115771\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.263986\tvalid_0's l2: 0.106553\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.253727\tvalid_0's l2: 0.0982998\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.244177\tvalid_0's l2: 0.0904721\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.244177\tvalid_0's l2: 0.0904721\n",
      "Starting TabNet for 0.46 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 24 and best_val_0_unsup_loss = 0.97826\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 3 and best_valid_accuracy = 0.5\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.46 on Iris\n",
      "False\n",
      "[17:46:33] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.46 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.751376\tvalid_0's l2: 0.719696\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.718527\tvalid_0's l2: 0.669969\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.698169\tvalid_0's l2: 0.621806\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.667785\tvalid_0's l2: 0.579302\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.64796\tvalid_0's l2: 0.537498\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.624751\tvalid_0's l2: 0.502986\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.602703\tvalid_0's l2: 0.471621\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.586809\tvalid_0's l2: 0.443107\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.571846\tvalid_0's l2: 0.417178\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.553571\tvalid_0's l2: 0.391089\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.531861\tvalid_0's l2: 0.364201\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.51563\tvalid_0's l2: 0.339661\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.500209\tvalid_0's l2: 0.317255\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.48869\tvalid_0's l2: 0.297287\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.477979\tvalid_0's l2: 0.279484\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.463566\tvalid_0's l2: 0.264267\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.454045\tvalid_0's l2: 0.249501\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.440627\tvalid_0's l2: 0.236244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.43176\tvalid_0's l2: 0.223395\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.419269\tvalid_0's l2: 0.211841\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.419269\tvalid_0's l2: 0.211841\n",
      "Starting TabNet for 0.47 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 5 and best_val_0_unsup_loss = 1.28058\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 65 with best_epoch = 45 and best_valid_accuracy = 0.75\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.47 on Iris\n",
      "False\n",
      "[17:46:35] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.47 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000151 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.608846\tvalid_0's l2: 0.57996\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.590982\tvalid_0's l2: 0.531762\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.574179\tvalid_0's l2: 0.491345\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.544174\tvalid_0's l2: 0.449023\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.528504\tvalid_0's l2: 0.415069\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.514663\tvalid_0's l2: 0.384849\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.498962\tvalid_0's l2: 0.359066\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.482498\tvalid_0's l2: 0.332638\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.474236\tvalid_0's l2: 0.311688\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.459398\tvalid_0's l2: 0.289435\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.444738\tvalid_0's l2: 0.26863\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.43417\tvalid_0's l2: 0.250279\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.417065\tvalid_0's l2: 0.23068\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.40411\tvalid_0's l2: 0.21465\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.394762\tvalid_0's l2: 0.200616\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.378447\tvalid_0's l2: 0.185703\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.367134\tvalid_0's l2: 0.172717\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.356836\tvalid_0's l2: 0.161896\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.348473\tvalid_0's l2: 0.152772\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.336335\tvalid_0's l2: 0.141721\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.336335\tvalid_0's l2: 0.141721\n",
      "Starting TabNet for 0.48 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 36 with best_epoch = 31 and best_val_0_unsup_loss = 0.98915\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 10 and best_valid_accuracy = 0.66667\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.48 on Iris\n",
      "False\n",
      "[17:46:36] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.48 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000033 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.622972\tvalid_0's l2: 0.586256\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.588858\tvalid_0's l2: 0.528705\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.557407\tvalid_0's l2: 0.476697\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.540614\tvalid_0's l2: 0.439168\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.51177\tvalid_0's l2: 0.395804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.489195\tvalid_0's l2: 0.368041\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.477682\tvalid_0's l2: 0.342986\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.455909\tvalid_0's l2: 0.313894\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.435224\tvalid_0's l2: 0.287559\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.415574\tvalid_0's l2: 0.263717\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.39533\tvalid_0's l2: 0.23858\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.376098\tvalid_0's l2: 0.215869\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.357828\tvalid_0's l2: 0.195348\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.340472\tvalid_0's l2: 0.176804\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.323983\tvalid_0's l2: 0.160046\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.308655\tvalid_0's l2: 0.145979\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.295995\tvalid_0's l2: 0.133266\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.29033\tvalid_0's l2: 0.123522\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.278726\tvalid_0's l2: 0.113121\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.266455\tvalid_0's l2: 0.105147\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.266455\tvalid_0's l2: 0.105147\n",
      "Starting TabNet for 0.49 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 46 with best_epoch = 41 and best_val_0_unsup_loss = 1.00938\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 55 with best_epoch = 35 and best_valid_accuracy = 0.76923\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.49 on Iris\n",
      "False\n",
      "[17:46:39] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.49 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.706631\tvalid_0's l2: 0.663823\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.674149\tvalid_0's l2: 0.604899\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.651536\tvalid_0's l2: 0.563269\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.62732\tvalid_0's l2: 0.513942\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.604315\tvalid_0's l2: 0.4696\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.587501\tvalid_0's l2: 0.441193\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.563051\tvalid_0's l2: 0.402879\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.539824\tvalid_0's l2: 0.368158\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.517758\tvalid_0's l2: 0.336687\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.496795\tvalid_0's l2: 0.308156\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.483786\tvalid_0's l2: 0.291202\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.471428\tvalid_0's l2: 0.275999\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.457315\tvalid_0's l2: 0.258932\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.445479\tvalid_0's l2: 0.243198\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.434975\tvalid_0's l2: 0.231296\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.419587\tvalid_0's l2: 0.214166\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.407214\tvalid_0's l2: 0.199141\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.393718\tvalid_0's l2: 0.18491\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.38246\tvalid_0's l2: 0.172489\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.37018\tvalid_0's l2: 0.160624\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.37018\tvalid_0's l2: 0.160624\n",
      "Starting TabNet for 0.5 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 17 and best_val_0_unsup_loss = 1.1466\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 39 with best_epoch = 19 and best_valid_accuracy = 0.61905\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.5 on Iris\n",
      "False\n",
      "[17:46:40] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.5 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.806746\tvalid_0's l2: 0.768455\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.766409\tvalid_0's l2: 0.693531\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.728088\tvalid_0's l2: 0.625912\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.691684\tvalid_0's l2: 0.564885\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.6571\tvalid_0's l2: 0.509809\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.628294\tvalid_0's l2: 0.466506\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.598396\tvalid_0's l2: 0.426835\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.574317\tvalid_0's l2: 0.390941\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.549654\tvalid_0's l2: 0.358073\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.528352\tvalid_0's l2: 0.328381\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.502277\tvalid_0's l2: 0.296396\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.477505\tvalid_0's l2: 0.26753\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.453973\tvalid_0's l2: 0.241478\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.431617\tvalid_0's l2: 0.217967\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.410378\tvalid_0's l2: 0.196748\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.397617\tvalid_0's l2: 0.185006\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.379599\tvalid_0's l2: 0.169504\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.362761\tvalid_0's l2: 0.154961\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.346766\tvalid_0's l2: 0.141756\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.333279\tvalid_0's l2: 0.129987\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.333279\tvalid_0's l2: 0.129987\n",
      "Starting TabNet for 0.51 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 37 with best_epoch = 32 and best_val_0_unsup_loss = 1.05425\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 58 with best_epoch = 38 and best_valid_accuracy = 0.7\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.51 on Iris\n",
      "False\n",
      "[17:46:42] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.51 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.66709\tvalid_0's l2: 0.627403\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.632307\tvalid_0's l2: 0.580914\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.610476\tvalid_0's l2: 0.536089\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.583227\tvalid_0's l2: 0.496906\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.5704\tvalid_0's l2: 0.459242\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.545849\tvalid_0's l2: 0.426871\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.527254\tvalid_0's l2: 0.391097\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.509589\tvalid_0's l2: 0.359056\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.492807\tvalid_0's l2: 0.330373\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.476805\tvalid_0's l2: 0.304089\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.462033\tvalid_0's l2: 0.286157\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.447993\tvalid_0's l2: 0.269933\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.435106\tvalid_0's l2: 0.254512\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.429349\tvalid_0's l2: 0.240927\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.423724\tvalid_0's l2: 0.228656\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.41333\tvalid_0's l2: 0.21576\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.403456\tvalid_0's l2: 0.204135\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.395285\tvalid_0's l2: 0.193274\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.387443\tvalid_0's l2: 0.183141\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.378639\tvalid_0's l2: 0.174086\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.378639\tvalid_0's l2: 0.174086\n",
      "Starting TabNet for 0.52 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 5 with best_epoch = 0 and best_val_0_unsup_loss = 1.47561\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n",
      "\n",
      "Early stopping occurred at epoch 54 with best_epoch = 34 and best_valid_accuracy = 0.78571\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.52 on Iris\n",
      "False\n",
      "[17:46:43] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.52 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.576473\tvalid_0's l2: 0.532168\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.547443\tvalid_0's l2: 0.489729\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.526006\tvalid_0's l2: 0.442388\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.505642\tvalid_0's l2: 0.399812\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.480327\tvalid_0's l2: 0.368402\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.459947\tvalid_0's l2: 0.332614\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.440585\tvalid_0's l2: 0.300387\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.425895\tvalid_0's l2: 0.286816\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.406913\tvalid_0's l2: 0.259495\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.39037\tvalid_0's l2: 0.234877\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.381111\tvalid_0's l2: 0.225223\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.364073\tvalid_0's l2: 0.205449\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.348025\tvalid_0's l2: 0.187584\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.33461\tvalid_0's l2: 0.171442\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.329827\tvalid_0's l2: 0.165032\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.317614\tvalid_0's l2: 0.15369\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.311999\tvalid_0's l2: 0.143519\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.298223\tvalid_0's l2: 0.133591\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.293063\tvalid_0's l2: 0.125034\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.284117\tvalid_0's l2: 0.120689\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.284117\tvalid_0's l2: 0.120689\n",
      "Starting TabNet for 0.53 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 9 and best_val_0_unsup_loss = 1.08463\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 43 with best_epoch = 23 and best_valid_accuracy = 0.7\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.53 on Iris\n",
      "False\n",
      "[17:46:44] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.53 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.436875\tvalid_0's l2: 0.413291\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.434409\tvalid_0's l2: 0.381245\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.403132\tvalid_0's l2: 0.352575\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.400881\tvalid_0's l2: 0.325276\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.372071\tvalid_0's l2: 0.300896\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.35749\tvalid_0's l2: 0.276056\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.34136\tvalid_0's l2: 0.250269\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.326036\tvalid_0's l2: 0.226947\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.30433\tvalid_0's l2: 0.211503\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.292584\tvalid_0's l2: 0.194223\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.28826\tvalid_0's l2: 0.182096\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.27986\tvalid_0's l2: 0.169428\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.277371\tvalid_0's l2: 0.15924\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.269252\tvalid_0's l2: 0.148726\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.264107\tvalid_0's l2: 0.138892\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.255546\tvalid_0's l2: 0.128222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.248413\tvalid_0's l2: 0.118693\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.238846\tvalid_0's l2: 0.110869\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.232146\tvalid_0's l2: 0.102675\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.223412\tvalid_0's l2: 0.0959646\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.223412\tvalid_0's l2: 0.0959646\n",
      "Starting TabNet for 0.54 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 9 and best_val_0_unsup_loss = 1.43634\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_valid_accuracy = 0.53846\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.54 on Iris\n",
      "False\n",
      "[17:46:45] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.54 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000043 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.701882\tvalid_0's l2: 0.666826\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.670122\tvalid_0's l2: 0.611834\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.639426\tvalid_0's l2: 0.553082\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.610592\tvalid_0's l2: 0.508253\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.582919\tvalid_0's l2: 0.4599\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.557746\tvalid_0's l2: 0.422199\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.533832\tvalid_0's l2: 0.388201\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.511113\tvalid_0's l2: 0.357543\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.48953\tvalid_0's l2: 0.3299\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.469027\tvalid_0's l2: 0.304975\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.449078\tvalid_0's l2: 0.283125\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.430127\tvalid_0's l2: 0.263453\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.412123\tvalid_0's l2: 0.245747\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.396966\tvalid_0's l2: 0.229868\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.379162\tvalid_0's l2: 0.210759\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.370218\tvalid_0's l2: 0.202138\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.353314\tvalid_0's l2: 0.185981\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.337255\tvalid_0's l2: 0.171322\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.322\tvalid_0's l2: 0.158021\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.307507\tvalid_0's l2: 0.145947\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.307507\tvalid_0's l2: 0.145947\n",
      "Starting TabNet for 0.55 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 23 and best_val_0_unsup_loss = 1.15418\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_accuracy = 0.55556\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.55 on Iris\n",
      "False\n",
      "[17:46:47] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.55 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000171 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.795437\tvalid_0's l2: 0.752595\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.755665\tvalid_0's l2: 0.679217\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.717881\tvalid_0's l2: 0.612993\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.681987\tvalid_0's l2: 0.553226\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.647888\tvalid_0's l2: 0.499287\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.617797\tvalid_0's l2: 0.454085\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.58921\tvalid_0's l2: 0.413142\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.562052\tvalid_0's l2: 0.376049\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.53663\tvalid_0's l2: 0.3428\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.512058\tvalid_0's l2: 0.312199\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.486612\tvalid_0's l2: 0.281764\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.462438\tvalid_0's l2: 0.254297\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.439473\tvalid_0's l2: 0.229508\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.417656\tvalid_0's l2: 0.207136\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.39693\tvalid_0's l2: 0.186945\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.378855\tvalid_0's l2: 0.170367\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.361684\tvalid_0's l2: 0.155337\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.345372\tvalid_0's l2: 0.141707\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.329875\tvalid_0's l2: 0.129345\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.315153\tvalid_0's l2: 0.11813\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.315153\tvalid_0's l2: 0.11813\n",
      "Starting TabNet for 0.56 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 9 and best_val_0_unsup_loss = 1.02736\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 3 and best_valid_accuracy = 0.42857\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.56 on Iris\n",
      "False\n",
      "[17:46:48] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.56 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000324 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.595163\tvalid_0's l2: 0.561494\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.565404\tvalid_0's l2: 0.506748\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.537134\tvalid_0's l2: 0.45734\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.510278\tvalid_0's l2: 0.41275\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.484764\tvalid_0's l2: 0.372507\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.460525\tvalid_0's l2: 0.336187\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.437499\tvalid_0's l2: 0.303409\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.415624\tvalid_0's l2: 0.273827\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.394843\tvalid_0's l2: 0.247128\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.375101\tvalid_0's l2: 0.223033\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.356346\tvalid_0's l2: 0.201288\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.338529\tvalid_0's l2: 0.181662\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.321602\tvalid_0's l2: 0.16395\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.305522\tvalid_0's l2: 0.147965\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.290246\tvalid_0's l2: 0.133538\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.275734\tvalid_0's l2: 0.120518\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.261947\tvalid_0's l2: 0.108768\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.24885\tvalid_0's l2: 0.098163\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.236407\tvalid_0's l2: 0.0885921\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.224587\tvalid_0's l2: 0.0799543\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.224587\tvalid_0's l2: 0.0799543\n",
      "Starting TabNet for 0.57 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 17 and best_val_0_unsup_loss = 1.07959\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 54 with best_epoch = 34 and best_valid_accuracy = 0.61111\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.57 on Iris\n",
      "False\n",
      "[17:46:49] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.57 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000160 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.751728\tvalid_0's l2: 0.712949\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.716284\tvalid_0's l2: 0.649763\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.682613\tvalid_0's l2: 0.592714\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.650625\tvalid_0's l2: 0.541205\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.625166\tvalid_0's l2: 0.490377\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.594482\tvalid_0's l2: 0.44315\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.565333\tvalid_0's l2: 0.400508\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.537641\tvalid_0's l2: 0.362005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.511333\tvalid_0's l2: 0.327239\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.486341\tvalid_0's l2: 0.295846\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.462941\tvalid_0's l2: 0.267922\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.44071\tvalid_0's l2: 0.242686\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.419591\tvalid_0's l2: 0.219877\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.399528\tvalid_0's l2: 0.199261\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.381355\tvalid_0's l2: 0.180922\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.364677\tvalid_0's l2: 0.165975\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.348833\tvalid_0's l2: 0.152496\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.333782\tvalid_0's l2: 0.140341\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.319483\tvalid_0's l2: 0.129381\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.305898\tvalid_0's l2: 0.119498\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.305898\tvalid_0's l2: 0.119498\n",
      "Starting TabNet for 0.58 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 18 and best_val_0_unsup_loss = 1.10038\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_valid_accuracy = 0.6875\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.58 on Iris\n",
      "False\n",
      "[17:46:50] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.58 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000060 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.471182\tvalid_0's l2: 0.435849\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.437806\tvalid_0's l2: 0.404108\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.421923\tvalid_0's l2: 0.364817\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.406834\tvalid_0's l2: 0.329483\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.380269\tvalid_0's l2: 0.305671\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.363473\tvalid_0's l2: 0.27586\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.349596\tvalid_0's l2: 0.249168\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.336413\tvalid_0's l2: 0.225278\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.32389\tvalid_0's l2: 0.203908\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.311992\tvalid_0's l2: 0.184803\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.300267\tvalid_0's l2: 0.167676\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.289127\tvalid_0's l2: 0.152381\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.276552\tvalid_0's l2: 0.141396\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.266525\tvalid_0's l2: 0.128892\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.256998\tvalid_0's l2: 0.117737\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.254013\tvalid_0's l2: 0.10922\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.241628\tvalid_0's l2: 0.0993321\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.229862\tvalid_0's l2: 0.0904209\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.218685\tvalid_0's l2: 0.0823906\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.216231\tvalid_0's l2: 0.0769209\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.216231\tvalid_0's l2: 0.0769209\n",
      "Starting TabNet for 0.59 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 25 and best_val_0_unsup_loss = 0.96729\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 65 with best_epoch = 45 and best_valid_accuracy = 0.57143\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.59 on Iris\n",
      "False\n",
      "[17:46:52] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.59 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000032 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.463066\tvalid_0's l2: 0.434019\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.444254\tvalid_0's l2: 0.400454\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.430745\tvalid_0's l2: 0.368232\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.412656\tvalid_0's l2: 0.34016\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.401015\tvalid_0's l2: 0.313157\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.379354\tvalid_0's l2: 0.288961\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.374448\tvalid_0's l2: 0.267421\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.350949\tvalid_0's l2: 0.246294\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.346424\tvalid_0's l2: 0.227935\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.324739\tvalid_0's l2: 0.20995\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.32072\tvalid_0's l2: 0.1948\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.301443\tvalid_0's l2: 0.179772\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.297733\tvalid_0's l2: 0.166823\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.279929\tvalid_0's l2: 0.153985\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.276505\tvalid_0's l2: 0.142916\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.273096\tvalid_0's l2: 0.132461\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.256836\tvalid_0's l2: 0.121847\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.253686\tvalid_0's l2: 0.112963\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.238675\tvalid_0's l2: 0.103902\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.235764\tvalid_0's l2: 0.0963545\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.235764\tvalid_0's l2: 0.0963545\n",
      "Starting TabNet for 0.6 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 10 and best_val_0_unsup_loss = 1.1626\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 5 and best_valid_accuracy = 0.66667\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.6 on Iris\n",
      "False\n",
      "[17:46:53] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.6 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000034 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.742667\tvalid_0's l2: 0.71428\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.704867\tvalid_0's l2: 0.649148\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.674038\tvalid_0's l2: 0.590427\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.64567\tvalid_0's l2: 0.537489\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.61872\tvalid_0's l2: 0.489766\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.590089\tvalid_0's l2: 0.448148\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.56289\tvalid_0's l2: 0.410483\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.540921\tvalid_0's l2: 0.376854\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.519279\tvalid_0's l2: 0.345685\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.499733\tvalid_0's l2: 0.317906\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.482255\tvalid_0's l2: 0.293861\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.465651\tvalid_0's l2: 0.272018\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.449877\tvalid_0's l2: 0.252169\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.434891\tvalid_0's l2: 0.234128\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.420656\tvalid_0's l2: 0.217724\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.409363\tvalid_0's l2: 0.206733\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.396427\tvalid_0's l2: 0.192267\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.384137\tvalid_0's l2: 0.179167\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.371611\tvalid_0's l2: 0.16635\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.360408\tvalid_0's l2: 0.155412\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.360408\tvalid_0's l2: 0.155412\n",
      "Starting TabNet for 0.61 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 11 and best_val_0_unsup_loss = 1.13192\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 14 and best_valid_accuracy = 0.55556\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.61 on Iris\n",
      "False\n",
      "[17:46:55] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.61 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000188 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.653037\tvalid_0's l2: 0.614146\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.620385\tvalid_0's l2: 0.554266\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.589366\tvalid_0's l2: 0.500225\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.559898\tvalid_0's l2: 0.451454\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.531903\tvalid_0's l2: 0.407437\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.505151\tvalid_0's l2: 0.367702\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.479736\tvalid_0's l2: 0.331841\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.455592\tvalid_0's l2: 0.299478\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.432655\tvalid_0's l2: 0.27027\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.410865\tvalid_0's l2: 0.243911\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.394412\tvalid_0's l2: 0.220896\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.37874\tvalid_0's l2: 0.200269\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.363852\tvalid_0's l2: 0.181816\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.349672\tvalid_0's l2: 0.165298\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.336201\tvalid_0's l2: 0.15054\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.322452\tvalid_0's l2: 0.137371\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.309391\tvalid_0's l2: 0.12567\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.299027\tvalid_0's l2: 0.115285\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.289731\tvalid_0's l2: 0.106078\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.276249\tvalid_0's l2: 0.0963004\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.276249\tvalid_0's l2: 0.0963004\n",
      "Starting TabNet for 0.62 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 11 and best_val_0_unsup_loss = 1.14838\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 7 and best_valid_accuracy = 0.5\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.62 on Iris\n",
      "False\n",
      "[17:46:56] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.62 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000173 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.717239\tvalid_0's l2: 0.67973\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.681377\tvalid_0's l2: 0.613457\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.647309\tvalid_0's l2: 0.553645\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.614943\tvalid_0's l2: 0.499664\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.584196\tvalid_0's l2: 0.450947\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.558929\tvalid_0's l2: 0.413163\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.534924\tvalid_0's l2: 0.378821\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.51212\tvalid_0's l2: 0.347596\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.490457\tvalid_0's l2: 0.319197\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.469876\tvalid_0's l2: 0.29336\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.446451\tvalid_0's l2: 0.266181\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.426413\tvalid_0's l2: 0.241664\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.407833\tvalid_0's l2: 0.219549\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.390182\tvalid_0's l2: 0.1996\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.373414\tvalid_0's l2: 0.181606\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.361638\tvalid_0's l2: 0.172125\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.350451\tvalid_0's l2: 0.163474\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.339824\tvalid_0's l2: 0.155577\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.329729\tvalid_0's l2: 0.148365\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.320323\tvalid_0's l2: 0.138275\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.320323\tvalid_0's l2: 0.138275\n",
      "Starting TabNet for 0.63 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 12 and best_val_0_unsup_loss = 1.21948\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 12 and best_valid_accuracy = 0.47368\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.63 on Iris\n",
      "False\n",
      "[17:46:57] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.63 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000032 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.554151\tvalid_0's l2: 0.511651\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.530606\tvalid_0's l2: 0.467511\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.509556\tvalid_0's l2: 0.427858\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.489559\tvalid_0's l2: 0.392246\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.470562\tvalid_0's l2: 0.360272\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.449995\tvalid_0's l2: 0.326937\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.430456\tvalid_0's l2: 0.297019\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.411894\tvalid_0's l2: 0.270174\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.39426\tvalid_0's l2: 0.246097\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.377508\tvalid_0's l2: 0.22451\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.362112\tvalid_0's l2: 0.205759\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.347485\tvalid_0's l2: 0.188941\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.331651\tvalid_0's l2: 0.172315\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.318406\tvalid_0's l2: 0.158708\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.304046\tvalid_0's l2: 0.145092\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.291362\tvalid_0's l2: 0.133648\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.279312\tvalid_0's l2: 0.123419\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.267864\tvalid_0's l2: 0.11428\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.256989\tvalid_0's l2: 0.106122\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.246657\tvalid_0's l2: 0.0988431\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.246657\tvalid_0's l2: 0.0988431\n",
      "Starting TabNet for 0.64 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 12 and best_val_0_unsup_loss = 1.23336\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 56 with best_epoch = 36 and best_valid_accuracy = 0.625\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.64 on Iris\n",
      "False\n",
      "[17:46:58] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.64 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000033 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.744337\tvalid_0's l2: 0.71481\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.711286\tvalid_0's l2: 0.653679\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.679889\tvalid_0's l2: 0.598487\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.650061\tvalid_0's l2: 0.548655\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.621725\tvalid_0's l2: 0.503661\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.59467\tvalid_0's l2: 0.462957\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.568969\tvalid_0's l2: 0.426202\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.544552\tvalid_0's l2: 0.39301\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.521357\tvalid_0's l2: 0.363037\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.499321\tvalid_0's l2: 0.335968\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.478522\tvalid_0's l2: 0.311598\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.458762\tvalid_0's l2: 0.289591\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.439991\tvalid_0's l2: 0.269717\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.422158\tvalid_0's l2: 0.251769\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.405217\tvalid_0's l2: 0.235559\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.388928\tvalid_0's l2: 0.220813\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.373454\tvalid_0's l2: 0.207491\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.358754\tvalid_0's l2: 0.195454\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.344789\tvalid_0's l2: 0.184578\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.331522\tvalid_0's l2: 0.174751\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.331522\tvalid_0's l2: 0.174751\n",
      "Starting TabNet for 0.65 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 13 and best_val_0_unsup_loss = 1.06425\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 37 with best_epoch = 17 and best_valid_accuracy = 0.71429\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.65 on Iris\n",
      "False\n",
      "[17:47:00] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.65 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000034 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.507565\tvalid_0's l2: 0.469404\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.48244\tvalid_0's l2: 0.42407\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.45857\tvalid_0's l2: 0.383136\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.435894\tvalid_0's l2: 0.346173\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.414352\tvalid_0's l2: 0.312795\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.393912\tvalid_0's l2: 0.282689\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.374494\tvalid_0's l2: 0.2555\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.356047\tvalid_0's l2: 0.230944\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.338523\tvalid_0's l2: 0.208765\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.32187\tvalid_0's l2: 0.188428\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.306028\tvalid_0's l2: 0.170333\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.293051\tvalid_0's l2: 0.15448\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.278642\tvalid_0's l2: 0.139668\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.266885\tvalid_0's l2: 0.126805\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.253778\tvalid_0's l2: 0.114676\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.241329\tvalid_0's l2: 0.103697\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.229502\tvalid_0's l2: 0.0937797\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.218266\tvalid_0's l2: 0.084821\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.207592\tvalid_0's l2: 0.0767278\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.197452\tvalid_0's l2: 0.0694163\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.197452\tvalid_0's l2: 0.0694163\n",
      "Starting TabNet for 0.66 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 20 and best_val_0_unsup_loss = 1.10456\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_valid_accuracy = 0.55556\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.66 on Iris\n",
      "False\n",
      "[17:47:01] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.66 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000166 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.678501\tvalid_0's l2: 0.624694\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.645164\tvalid_0's l2: 0.564778\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.613494\tvalid_0's l2: 0.510657\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.583408\tvalid_0's l2: 0.461766\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.554825\tvalid_0's l2: 0.4176\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.527672\tvalid_0's l2: 0.377698\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.501877\tvalid_0's l2: 0.341648\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.477371\tvalid_0's l2: 0.309075\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.454091\tvalid_0's l2: 0.279643\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.431975\tvalid_0's l2: 0.253048\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.410376\tvalid_0's l2: 0.228376\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.389857\tvalid_0's l2: 0.206109\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.370364\tvalid_0's l2: 0.186013\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.351846\tvalid_0's l2: 0.167877\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.334254\tvalid_0's l2: 0.151509\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.317514\tvalid_0's l2: 0.136735\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.30161\tvalid_0's l2: 0.123402\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.286502\tvalid_0's l2: 0.111369\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.27215\tvalid_0's l2: 0.100509\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.258514\tvalid_0's l2: 0.090708\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.258514\tvalid_0's l2: 0.090708\n",
      "Starting TabNet for 0.67 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 28 and best_val_0_unsup_loss = 1.05972\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 63 with best_epoch = 43 and best_valid_accuracy = 0.80952\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.67 on Iris\n",
      "False\n",
      "[17:47:03] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.67 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000032 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.540483\tvalid_0's l2: 0.517484\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.51273\tvalid_0's l2: 0.466984\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.486364\tvalid_0's l2: 0.421413\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.461317\tvalid_0's l2: 0.38029\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.437522\tvalid_0's l2: 0.343179\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.415227\tvalid_0's l2: 0.309393\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.394048\tvalid_0's l2: 0.278918\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.373928\tvalid_0's l2: 0.25143\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.354813\tvalid_0's l2: 0.226637\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.336654\tvalid_0's l2: 0.204275\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.319639\tvalid_0's l2: 0.184353\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.303474\tvalid_0's l2: 0.166374\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.288118\tvalid_0's l2: 0.150149\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.273529\tvalid_0's l2: 0.135505\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.25967\tvalid_0's l2: 0.12229\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.246639\tvalid_0's l2: 0.110366\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.23426\tvalid_0's l2: 0.0996046\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.22266\tvalid_0's l2: 0.089895\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.211624\tvalid_0's l2: 0.0811317\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.20114\tvalid_0's l2: 0.0732228\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.20114\tvalid_0's l2: 0.0732228\n",
      "Starting TabNet for 0.68 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 10 with best_epoch = 5 and best_val_0_unsup_loss = 1.18587\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 38 with best_epoch = 18 and best_valid_accuracy = 0.57895\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.68 on Iris\n",
      "False\n",
      "[17:47:04] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.68 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000052 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.678095\tvalid_0's l2: 0.626434\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.643714\tvalid_0's l2: 0.565326\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.611052\tvalid_0's l2: 0.51018\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.580024\tvalid_0's l2: 0.460413\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.550546\tvalid_0's l2: 0.415502\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.52322\tvalid_0's l2: 0.375297\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.49726\tvalid_0's l2: 0.338997\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.472598\tvalid_0's l2: 0.306223\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.449169\tvalid_0's l2: 0.27663\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.426911\tvalid_0's l2: 0.24991\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.40532\tvalid_0's l2: 0.225536\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.384807\tvalid_0's l2: 0.203539\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.365321\tvalid_0's l2: 0.183688\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.346809\tvalid_0's l2: 0.165773\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.329222\tvalid_0's l2: 0.149606\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.312843\tvalid_0's l2: 0.135021\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.297283\tvalid_0's l2: 0.121857\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.282501\tvalid_0's l2: 0.109978\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.268458\tvalid_0's l2: 0.0992563\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.255117\tvalid_0's l2: 0.0895801\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.255117\tvalid_0's l2: 0.0895801\n",
      "Starting TabNet for 0.69 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 18 and best_val_0_unsup_loss = 1.15732\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 9 and best_valid_accuracy = 0.70588\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.69 on Iris\n",
      "False\n",
      "[17:47:05] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.69 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000032 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.834028\tvalid_0's l2: 0.793353\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.793082\tvalid_0's l2: 0.717404\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.754184\tvalid_0's l2: 0.648793\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.719034\tvalid_0's l2: 0.590449\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.685642\tvalid_0's l2: 0.537784\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.651462\tvalid_0's l2: 0.485577\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.618991\tvalid_0's l2: 0.438453\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.588143\tvalid_0's l2: 0.395918\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.558838\tvalid_0's l2: 0.357524\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.530998\tvalid_0's l2: 0.322869\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.50425\tvalid_0's l2: 0.291253\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.478839\tvalid_0's l2: 0.262732\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.454699\tvalid_0's l2: 0.237002\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.431766\tvalid_0's l2: 0.21379\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.40998\tvalid_0's l2: 0.192852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.391402\tvalid_0's l2: 0.176209\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.373753\tvalid_0's l2: 0.161191\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.356987\tvalid_0's l2: 0.14764\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.341058\tvalid_0's l2: 0.135413\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.324548\tvalid_0's l2: 0.122861\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.324548\tvalid_0's l2: 0.122861\n",
      "Starting TabNet for 0.7 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 8 with best_epoch = 3 and best_val_0_unsup_loss = 1.37689\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 46 with best_epoch = 26 and best_valid_accuracy = 0.83333\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.7 on Iris\n",
      "False\n",
      "[17:47:07] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.7 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000032 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.523278\tvalid_0's l2: 0.492501\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.502211\tvalid_0's l2: 0.445144\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.482197\tvalid_0's l2: 0.402815\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.463183\tvalid_0's l2: 0.365005\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.44512\tvalid_0's l2: 0.331253\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.42741\tvalid_0's l2: 0.301137\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.410585\tvalid_0's l2: 0.274291\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.394601\tvalid_0's l2: 0.25038\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.379416\tvalid_0's l2: 0.229102\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.364991\tvalid_0's l2: 0.210185\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.354328\tvalid_0's l2: 0.19399\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.344209\tvalid_0's l2: 0.181941\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.334542\tvalid_0's l2: 0.169182\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.325367\tvalid_0's l2: 0.159794\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.316602\tvalid_0's l2: 0.149796\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.308029\tvalid_0's l2: 0.140776\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.299884\tvalid_0's l2: 0.132935\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.292147\tvalid_0's l2: 0.126143\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.284797\tvalid_0's l2: 0.120283\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.277814\tvalid_0's l2: 0.115251\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.277814\tvalid_0's l2: 0.115251\n",
      "Starting TabNet for 0.71 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 6 and best_val_0_unsup_loss = 1.34893\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 57 with best_epoch = 37 and best_valid_accuracy = 0.9\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.71 on Iris\n",
      "False\n",
      "[17:47:08] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.71 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.639165\tvalid_0's l2: 0.600648\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.610982\tvalid_0's l2: 0.543387\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.584209\tvalid_0's l2: 0.491973\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.558774\tvalid_0's l2: 0.445823\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.534611\tvalid_0's l2: 0.404412\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.511835\tvalid_0's l2: 0.367479\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.490197\tvalid_0's l2: 0.334371\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.469443\tvalid_0's l2: 0.304458\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.449877\tvalid_0's l2: 0.277837\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.43129\tvalid_0's l2: 0.254008\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.413652\tvalid_0's l2: 0.232688\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.396895\tvalid_0's l2: 0.213624\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.380976\tvalid_0's l2: 0.196586\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.365854\tvalid_0's l2: 0.181368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.351487\tvalid_0's l2: 0.167785\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.3376\tvalid_0's l2: 0.155427\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.324408\tvalid_0's l2: 0.144412\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.311876\tvalid_0's l2: 0.134603\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.300116\tvalid_0's l2: 0.126025\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.288789\tvalid_0's l2: 0.118255\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.288789\tvalid_0's l2: 0.118255\n",
      "Starting TabNet for 0.72 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 7 and best_val_0_unsup_loss = 1.24746\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 42 with best_epoch = 22 and best_valid_accuracy = 0.64706\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.72 on Iris\n",
      "False\n",
      "[17:47:09] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.72 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000619 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.492039\tvalid_0's l2: 0.441093\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.467642\tvalid_0's l2: 0.39981\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.444491\tvalid_0's l2: 0.362449\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.423055\tvalid_0's l2: 0.327601\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.401734\tvalid_0's l2: 0.295623\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.382152\tvalid_0's l2: 0.267391\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.363659\tvalid_0's l2: 0.242253\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.345895\tvalid_0's l2: 0.219142\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.329018\tvalid_0's l2: 0.19826\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.313047\tvalid_0's l2: 0.179651\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.296982\tvalid_0's l2: 0.162432\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.282422\tvalid_0's l2: 0.147128\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.267936\tvalid_0's l2: 0.133057\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.254179\tvalid_0's l2: 0.120344\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.241508\tvalid_0's l2: 0.108655\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.229659\tvalid_0's l2: 0.0983471\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.217727\tvalid_0's l2: 0.0890844\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.207199\tvalid_0's l2: 0.0806486\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.198917\tvalid_0's l2: 0.0731294\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.189596\tvalid_0's l2: 0.0664409\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.189596\tvalid_0's l2: 0.0664409\n",
      "Starting TabNet for 0.73 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 15 and best_val_0_unsup_loss = 1.12799\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 1 and best_valid_accuracy = 0.44444\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.73 on Iris\n",
      "False\n",
      "[17:47:11] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.73 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000032 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.704356\tvalid_0's l2: 0.662808\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.673305\tvalid_0's l2: 0.600333\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.643807\tvalid_0's l2: 0.544156\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.612968\tvalid_0's l2: 0.492238\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.586286\tvalid_0's l2: 0.446571\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.560673\tvalid_0's l2: 0.405076\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.536341\tvalid_0's l2: 0.367827\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.513225\tvalid_0's l2: 0.334401\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.491265\tvalid_0's l2: 0.304416\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.470404\tvalid_0's l2: 0.277527\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.448026\tvalid_0's l2: 0.251436\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.426767\tvalid_0's l2: 0.227879\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.406572\tvalid_0's l2: 0.20661\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.387386\tvalid_0's l2: 0.187406\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.369159\tvalid_0's l2: 0.170065\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.353681\tvalid_0's l2: 0.155689\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.338978\tvalid_0's l2: 0.142826\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.325009\tvalid_0's l2: 0.131323\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.310181\tvalid_0's l2: 0.11966\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.297553\tvalid_0's l2: 0.11023\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.297553\tvalid_0's l2: 0.11023\n",
      "Starting TabNet for 0.74 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 19 and best_val_0_unsup_loss = 1.0784\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 58 with best_epoch = 38 and best_valid_accuracy = 0.61538\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.74 on Iris\n",
      "False\n",
      "[17:47:12] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.74 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000036 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.535597\tvalid_0's l2: 0.501698\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.508817\tvalid_0's l2: 0.452783\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.483376\tvalid_0's l2: 0.408636\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.459207\tvalid_0's l2: 0.368794\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.436247\tvalid_0's l2: 0.332837\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.414435\tvalid_0's l2: 0.300385\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.393713\tvalid_0's l2: 0.271098\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.374027\tvalid_0's l2: 0.244666\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.355326\tvalid_0's l2: 0.220811\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.33756\tvalid_0's l2: 0.199282\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.320682\tvalid_0's l2: 0.179852\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.304647\tvalid_0's l2: 0.162316\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.289415\tvalid_0's l2: 0.14649\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.274944\tvalid_0's l2: 0.132208\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.261197\tvalid_0's l2: 0.119317\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.248137\tvalid_0's l2: 0.107684\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.23573\tvalid_0's l2: 0.0971847\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.223944\tvalid_0's l2: 0.0877092\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.212747\tvalid_0's l2: 0.0791576\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.202109\tvalid_0's l2: 0.0714397\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.202109\tvalid_0's l2: 0.0714397\n",
      "Starting TabNet for 0.75 on Iris\n",
      "False\n",
      "\n",
      "Early stopping occurred at epoch 5 with best_epoch = 0 and best_val_0_unsup_loss = 1.77894\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from unsupervised pretraining\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 0 and best_valid_accuracy = 0.66667\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.75 on Iris\n",
      "False\n",
      "[17:47:13] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.75 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000064 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.586057\tvalid_0's l2: 0.550678\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.557262\tvalid_0's l2: 0.497923\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.529906\tvalid_0's l2: 0.450266\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.503918\tvalid_0's l2: 0.407213\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.479229\tvalid_0's l2: 0.368317\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.457854\tvalid_0's l2: 0.335603\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.435101\tvalid_0's l2: 0.303615\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.413713\tvalid_0's l2: 0.275362\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.393069\tvalid_0's l2: 0.249161\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.373456\tvalid_0's l2: 0.225487\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.355216\tvalid_0's l2: 0.204324\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.340906\tvalid_0's l2: 0.185726\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.330442\tvalid_0's l2: 0.171902\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.320501\tvalid_0's l2: 0.159515\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.308993\tvalid_0's l2: 0.14624\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.295477\tvalid_0's l2: 0.133492\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.282636\tvalid_0's l2: 0.121982\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.270745\tvalid_0's l2: 0.111809\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.259449\tvalid_0's l2: 0.102608\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.248718\tvalid_0's l2: 0.0942853\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.248718\tvalid_0's l2: 0.0942853\n",
      "Starting TabNet for 0.76 on Iris\n",
      "False\n",
      "TabNet failed on Iris for value 0.76\n",
      "Starting XGBoost for 0.76 on Iris\n",
      "False\n",
      "[17:47:13] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.76 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.752942\tvalid_0's l2: 0.719202\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.715336\tvalid_0's l2: 0.649697\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.67961\tvalid_0's l2: 0.586941\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.64567\tvalid_0's l2: 0.530275\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.613428\tvalid_0's l2: 0.479108\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.584849\tvalid_0's l2: 0.435616\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.5577\tvalid_0's l2: 0.396368\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.531908\tvalid_0's l2: 0.360951\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.507405\tvalid_0's l2: 0.328992\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.484128\tvalid_0's l2: 0.300152\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.462257\tvalid_0's l2: 0.274414\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.441479\tvalid_0's l2: 0.251174\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.42174\tvalid_0's l2: 0.230191\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.402988\tvalid_0's l2: 0.211244\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.385173\tvalid_0's l2: 0.194135\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.366648\tvalid_0's l2: 0.17657\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.350399\tvalid_0's l2: 0.161635\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.333756\tvalid_0's l2: 0.147102\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.317792\tvalid_0's l2: 0.134001\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.302723\tvalid_0's l2: 0.122068\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.302723\tvalid_0's l2: 0.122068\n",
      "Starting TabNet for 0.77 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 31 with best_epoch = 26 and best_val_0_unsup_loss = 1.02108\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 4 and best_valid_accuracy = 0.5\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.77 on Iris\n",
      "False\n",
      "[17:47:14] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.77 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000032 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.598713\tvalid_0's l2: 0.563206\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.569491\tvalid_0's l2: 0.50967\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.541729\tvalid_0's l2: 0.461287\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.515356\tvalid_0's l2: 0.417558\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.490302\tvalid_0's l2: 0.378033\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.4665\tvalid_0's l2: 0.342305\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.443889\tvalid_0's l2: 0.310006\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.422408\tvalid_0's l2: 0.280805\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.402001\tvalid_0's l2: 0.254402\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.381162\tvalid_0's l2: 0.229541\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.361927\tvalid_0's l2: 0.207198\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.343653\tvalid_0's l2: 0.187032\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.326294\tvalid_0's l2: 0.168831\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.309802\tvalid_0's l2: 0.152403\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.294135\tvalid_0's l2: 0.137576\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.279227\tvalid_0's l2: 0.123963\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.265065\tvalid_0's l2: 0.111687\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.25161\tvalid_0's l2: 0.100619\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.237059\tvalid_0's l2: 0.0908406\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.225131\tvalid_0's l2: 0.0817153\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.225131\tvalid_0's l2: 0.0817153\n",
      "Starting TabNet for 0.78 on Iris\n",
      "False\n",
      "\n",
      "Early stopping occurred at epoch 5 with best_epoch = 0 and best_val_0_unsup_loss = 2.11843\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from unsupervised pretraining\n",
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 3 and best_valid_accuracy = 0.58333\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.78 on Iris\n",
      "False\n",
      "[17:47:15] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.78 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000032 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.53143\tvalid_0's l2: 0.478705\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.505172\tvalid_0's l2: 0.432565\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.479371\tvalid_0's l2: 0.390344\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.454876\tvalid_0's l2: 0.352222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.431606\tvalid_0's l2: 0.317821\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.409508\tvalid_0's l2: 0.286702\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.388515\tvalid_0's l2: 0.258625\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.368584\tvalid_0's l2: 0.233281\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.350345\tvalid_0's l2: 0.210695\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.332372\tvalid_0's l2: 0.190037\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.315691\tvalid_0's l2: 0.171347\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.299844\tvalid_0's l2: 0.154488\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.28479\tvalid_0's l2: 0.13928\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.270489\tvalid_0's l2: 0.125563\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.256902\tvalid_0's l2: 0.11319\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.244179\tvalid_0's l2: 0.102061\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.232092\tvalid_0's l2: 0.0920224\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.22061\tvalid_0's l2: 0.0829673\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.209701\tvalid_0's l2: 0.0747996\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.199338\tvalid_0's l2: 0.0674325\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.199338\tvalid_0's l2: 0.0674325\n",
      "Starting TabNet for 0.79 on Iris\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 22 and best_val_0_unsup_loss = 1.13072\n",
      "Best weights from best epoch are automatically used!\n",
      "Loading weights from unsupervised pretraining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_dims changed from [] to [150, 35, 23, 43, 22]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_emb_dim changed from 1 to 3\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: cat_idxs changed from [] to [0, 1, 2, 3, 4]\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:97: UserWarning: Pretraining: mask_type changed from sparsemax to entmax\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_valid_accuracy = 0.64286\n",
      "Best weights from best epoch are automatically used!\n",
      "Starting XGBoost for 0.79 on Iris\n",
      "False\n",
      "[17:47:17] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Starting LightGBM for 0.79 on Iris\n",
      "False\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000035 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's l1: 0.556478\tvalid_0's l2: 0.528467\n",
      "Training until validation scores don't improve for 5 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's l1: 0.531285\tvalid_0's l2: 0.482032\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's l1: 0.507353\tvalid_0's l2: 0.440127\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's l1: 0.484617\tvalid_0's l2: 0.40231\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's l1: 0.463017\tvalid_0's l2: 0.368181\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's l1: 0.442498\tvalid_0's l2: 0.337382\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's l1: 0.423005\tvalid_0's l2: 0.309587\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's l1: 0.404486\tvalid_0's l2: 0.284504\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's l1: 0.386893\tvalid_0's l2: 0.261868\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's l1: 0.37018\tvalid_0's l2: 0.241441\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's l1: 0.354303\tvalid_0's l2: 0.223006\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's l1: 0.339219\tvalid_0's l2: 0.206371\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's l1: 0.32489\tvalid_0's l2: 0.191358\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's l1: 0.311277\tvalid_0's l2: 0.17781\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's l1: 0.298345\tvalid_0's l2: 0.165584\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's l1: 0.286059\tvalid_0's l2: 0.154552\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's l1: 0.274388\tvalid_0's l2: 0.144596\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's l1: 0.2633\tvalid_0's l2: 0.135611\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's l1: 0.252766\tvalid_0's l2: 0.127504\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's l1: 0.24276\tvalid_0's l2: 0.120188\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalid_0's l1: 0.24276\tvalid_0's l2: 0.120188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomadams/opt/anaconda3/envs/TabNet/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#datasets = {'Churn':churn,'Bank Marketing':bank_marketing,'Income':adult}\n",
    "datasets = {'Iris':iris}\n",
    "#test_probs = [0.4]\n",
    "test_probs = [x/100 for x in list(range(1,80))]\n",
    "\n",
    "for d in datasets:\n",
    "    \n",
    "    data = datasets[d].copy()\n",
    "    \n",
    "    binary = data.nunique()[-1] == 2\n",
    "    \n",
    "    tabnet_scores = []\n",
    "    xgboost_scores = []\n",
    "    lightgbm_scores = []\n",
    "    tprobs = []\n",
    "    xprobs = []\n",
    "    lprobs = []\n",
    "    \n",
    "    for t in test_probs:\n",
    "            \n",
    "        try:\n",
    "            print('Starting TabNet for ' + str(t) + ' on ' + d)\n",
    "            tabnet_score = run_tabnet(data,t)\n",
    "            tabnet_scores.append(tabnet_score)\n",
    "            tprobs.append(t)\n",
    "        except:\n",
    "            print('TabNet failed on ' + d + ' for value ' + str(t))\n",
    "            \n",
    "        try:\n",
    "            print('Starting XGBoost for ' + str(t) + ' on ' + d)\n",
    "            xgboost_score = run_xgboost(data,t)\n",
    "            xgboost_scores.append(xgboost_score)\n",
    "            xprobs.append(t)\n",
    "        except:\n",
    "            print('XGBoost failed on ' + d + ' for value ' + str(t))\n",
    "            \n",
    "        try:\n",
    "            print('Starting LightGBM for ' + str(t) + ' on ' + d)\n",
    "            lightgbm_score = run_lightgbm(data,t)\n",
    "            lightgbm_scores.append(lightgbm_score)\n",
    "            lprobs.append(t)\n",
    "        except:\n",
    "            print('LightGBM failed on ' + d + ' for value ' + str(t))\n",
    "    \n",
    "    from matplotlib.pyplot import figure\n",
    "    figure(figsize=(15, 6), dpi=80)\n",
    "    \n",
    "    plt.title(d)\n",
    "    plt.plot(tprobs,tabnet_scores,color='green',label='TabNet')\n",
    "    plt.plot(xprobs,xgboost_scores,color='red',label='XGBoost')\n",
    "    plt.plot(lprobs,lightgbm_scores,color='blue',label='LightGBM')\n",
    "    \n",
    "    plt.scatter(tprobs,tabnet_scores,color='green')\n",
    "    plt.scatter(xprobs,xgboost_scores,color='red')\n",
    "    plt.scatter(lprobs,lightgbm_scores,color='blue')\n",
    "    \n",
    "    plt.xlabel('Proportion of dataset used for training')\n",
    "    \n",
    "    ylabel = 'AUROC Score' if binary else 'Accuracy'\n",
    "    \n",
    "    plt.ylabel(ylabel)\n",
    "    \n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig(d + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f7f3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
